{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Reinforcement_Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3Eup/86A6dmjNu7F8NNpW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckkissane/deep_learning_curriculum/blob/ch-6-sol/solutions/6_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement PPO and run it on Procgen. If you use the environments in easy mode, you'll be able to train on 1 GPU, which means you won't need to worry about data parallelism (though making a data-parallel implementation could be instructive)."
      ],
      "metadata": {
        "id": "GXkPqrmT1om9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install procgen"
      ],
      "metadata": {
        "id": "6TM_viSmvSx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "import gym\n",
        "from gym import spaces\n",
        "from procgen import ProcgenEnv\n",
        "import random\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "from abc import ABC, abstractmethod\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ok4ywweHS0Pz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "oI3vtd_fsUpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_global_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "wNGmN653r45-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### storage for trajectory info"
      ],
      "metadata": {
        "id": "f2Y229iZtSdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Storage():\n",
        "\n",
        "    def __init__(self, obs_shape, num_steps, num_envs, device):\n",
        "        self.obs_shape = obs_shape\n",
        "        self.num_steps = num_steps\n",
        "        self.num_envs = num_envs\n",
        "        self.device = device\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.obs_batch = torch.zeros(self.num_steps+1, self.num_envs, *self.obs_shape)\n",
        "        self.act_batch = torch.zeros(self.num_steps, self.num_envs)\n",
        "        self.rew_batch = torch.zeros(self.num_steps, self.num_envs)\n",
        "        self.done_batch = torch.zeros(self.num_steps, self.num_envs)\n",
        "        self.log_prob_act_batch = torch.zeros(self.num_steps, self.num_envs)\n",
        "        self.value_batch = torch.zeros(self.num_steps+1, self.num_envs)\n",
        "        self.return_batch = torch.zeros(self.num_steps, self.num_envs)\n",
        "        self.adv_batch = torch.zeros(self.num_steps, self.num_envs)\n",
        "        self.info_batch = deque(maxlen=self.num_steps)\n",
        "        self.step = 0\n",
        "\n",
        "    def store(self, obs, act, rew, done, info, log_prob_act, value):\n",
        "        self.obs_batch[self.step] = torch.from_numpy(obs.copy())\n",
        "        self.act_batch[self.step] = torch.from_numpy(act.copy())\n",
        "        self.rew_batch[self.step] = torch.from_numpy(rew.copy())\n",
        "        self.done_batch[self.step] = torch.from_numpy(done.copy())\n",
        "        self.log_prob_act_batch[self.step] = torch.from_numpy(log_prob_act.copy())\n",
        "        self.value_batch[self.step] = torch.from_numpy(value.copy())\n",
        "        self.info_batch.append(info)\n",
        "\n",
        "        self.step = (self.step + 1) % self.num_steps\n",
        "\n",
        "    def store_last(self, last_obs, last_value):\n",
        "        self.obs_batch[-1] = torch.from_numpy(last_obs.copy())\n",
        "        self.value_batch[-1] = torch.from_numpy(last_value.copy())\n",
        "\n",
        "    def compute_estimates(self, gamma=0.99, lmbda=0.95):\n",
        "        rew_batch = self.rew_batch\n",
        "        # use_gae\n",
        "        A = 0\n",
        "        for i in reversed(range(self.num_steps)):\n",
        "            rew = rew_batch[i]\n",
        "            done = self.done_batch[i]\n",
        "            value = self.value_batch[i]\n",
        "            next_value = self.value_batch[i+1]\n",
        "\n",
        "            delta = (rew + gamma * next_value * (1 - done)) - value\n",
        "            self.adv_batch[i] = A = gamma * lmbda * A * (1 - done) + delta\n",
        "\n",
        "        self.return_batch = self.adv_batch + self.value_batch[:-1]\n",
        "        # normalize_adv\n",
        "        adv_mean = torch.mean(self.adv_batch)\n",
        "        adv_std = torch.std(self.adv_batch)\n",
        "        print(f\"adv_mean: {adv_mean}\")\n",
        "        print(f\"adv_std: {adv_std}\")\n",
        "        self.adv_batch = (self.adv_batch - adv_mean) / (adv_std + 1e-8)\n",
        "\n",
        "\n",
        "    def fetch_train_generator(self, mini_batch_size=None):\n",
        "        batch_size = self.num_steps * self.num_envs\n",
        "        if mini_batch_size is None:\n",
        "            mini_batch_size = batch_size\n",
        "        sampler = BatchSampler(SubsetRandomSampler(range(batch_size)),\n",
        "                                mini_batch_size,\n",
        "                                drop_last=True)\n",
        "        for indices in sampler:\n",
        "            obs_batch = torch.FloatTensor(self.obs_batch[:-1]).reshape(-1, *self.obs_shape)[indices].to(self.device)\n",
        "            act_batch = torch.FloatTensor(self.act_batch).reshape(-1)[indices].to(self.device)\n",
        "            done_batch = torch.FloatTensor(self.done_batch).reshape(-1)[indices].to(self.device)\n",
        "            log_prob_act_batch = torch.FloatTensor(self.log_prob_act_batch).reshape(-1)[indices].to(self.device)\n",
        "            value_batch = torch.FloatTensor(self.value_batch[:-1]).reshape(-1)[indices].to(self.device)\n",
        "            return_batch = torch.FloatTensor(self.return_batch).reshape(-1)[indices].to(self.device)\n",
        "            adv_batch = torch.FloatTensor(self.adv_batch).reshape(-1)[indices].to(self.device)\n",
        "            yield obs_batch, act_batch, done_batch, log_prob_act_batch, value_batch, return_batch, adv_batch\n",
        "\n",
        "    def fetch_log_data(self):\n",
        "        if 'env_reward' in self.info_batch[0][0]:\n",
        "            rew_batch = []\n",
        "            for step in range(self.num_steps):\n",
        "                infos = self.info_batch[step]\n",
        "                rew_batch.append([info['env_reward'] for info in infos])\n",
        "            rew_batch = np.array(rew_batch)\n",
        "        else:\n",
        "            rew_batch = self.rew_batch.numpy()\n",
        "        if 'env_done' in self.info_batch[0][0]:\n",
        "            done_batch = []\n",
        "            for step in range(self.num_steps):\n",
        "                infos = self.info_batch[step]\n",
        "                done_batch.append([info['env_done'] for info in infos])\n",
        "            done_batch = np.array(done_batch)\n",
        "        else:\n",
        "            done_batch = self.done_batch.numpy()\n",
        "        return rew_batch, done_batch"
      ],
      "metadata": {
        "id": "Qax9OH26tVyb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### logger"
      ],
      "metadata": {
        "id": "F-EpH8-UtgYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger(object):\n",
        "    \n",
        "    def __init__(self, n_envs, logdir):\n",
        "        self.start_time = time.time()\n",
        "        self.n_envs = n_envs\n",
        "        self.logdir = logdir\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        for _ in range(n_envs):\n",
        "            self.episode_rewards.append([])\n",
        "        self.episode_len_buffer = deque(maxlen = 40)\n",
        "        self.episode_reward_buffer = deque(maxlen = 40)\n",
        "        \n",
        "        self.log = pd.DataFrame(columns = ['timesteps', 'wall_time', 'num_episodes',\n",
        "                               'max_episode_rewards', 'mean_episode_rewards','min_episode_rewards',\n",
        "                               'max_episode_len', 'mean_episode_len', 'min_episode_len'])\n",
        "        self.timesteps = 0\n",
        "        self.num_episodes = 0\n",
        "\n",
        "    def feed(self, rew_batch, done_batch):\n",
        "        steps = rew_batch.shape[0]\n",
        "        rew_batch = rew_batch.T\n",
        "        done_batch = done_batch.T\n",
        "\n",
        "        for i in range(self.n_envs):\n",
        "            for j in range(steps):\n",
        "                self.episode_rewards[i].append(rew_batch[i][j])\n",
        "                if done_batch[i][j]:\n",
        "                    self.episode_len_buffer.append(len(self.episode_rewards[i]))\n",
        "                    self.episode_reward_buffer.append(np.sum(self.episode_rewards[i]))\n",
        "                    self.episode_rewards[i] = []\n",
        "                    self.num_episodes += 1\n",
        "        self.timesteps += (self.n_envs * steps)\n",
        "\n",
        "    def write_summary(self, summary):\n",
        "        for key, value in summary.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    def dump(self):\n",
        "        wall_time = time.time() - self.start_time\n",
        "        if self.num_episodes > 0:\n",
        "            episode_statistics = self._get_episode_statistics()\n",
        "            episode_statistics_list = list(episode_statistics.values())\n",
        "        else:\n",
        "            episode_statistics_list = [None] * 6\n",
        "        log = [self.timesteps] + [wall_time] + [self.num_episodes] + episode_statistics_list\n",
        "        self.log.loc[len(self.log)] = log\n",
        "\n",
        "        with open(self.logdir + '/log.csv', 'w') as f:\n",
        "            self.log.to_csv(f, index = False)\n",
        "        print(self.log.loc[len(self.log)-1])\n",
        "\n",
        "    def _get_episode_statistics(self):\n",
        "        episode_statistics = {}\n",
        "        episode_statistics['Rewards/max_episodes']  = np.max(self.episode_reward_buffer)\n",
        "        episode_statistics['Rewards/mean_episodes'] = np.mean(self.episode_reward_buffer)\n",
        "        episode_statistics['Rewards/min_episodes']  = np.min(self.episode_reward_buffer)\n",
        "        episode_statistics['Len/max_episodes']  = np.max(self.episode_len_buffer)\n",
        "        episode_statistics['Len/mean_episodes'] = np.mean(self.episode_len_buffer)\n",
        "        episode_statistics['Len/min_episodes']  = np.min(self.episode_len_buffer)\n",
        "        return episode_statistics"
      ],
      "metadata": {
        "id": "7z6TBW4Qti8i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### impala model definition "
      ],
      "metadata": {
        "id": "uq7tVgLjs5V9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(x)\n",
        "        out = self.conv1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + x\n",
        "\n",
        "class ImpalaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.res1 = ResidualBlock(out_channels)\n",
        "        self.res2 = ResidualBlock(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class ImpalaModel(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=256)\n",
        "\n",
        "        self.output_dim = 256\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        x = F.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ybpuqpa8s7w2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical policy definition"
      ],
      "metadata": {
        "id": "1LpcV5L5shJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalPolicy(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedder,\n",
        "                 action_size):\n",
        "        \"\"\"\n",
        "        embedder: (torch.Tensor) model to extract the embedding for observation\n",
        "        action_size: number of the categorical actions\n",
        "        \"\"\" \n",
        "        super().__init__()\n",
        "        self.embedder = embedder\n",
        "        self.fc_policy = nn.Linear(self.embedder.output_dim, action_size)\n",
        "        self.fc_value = nn.Linear(self.embedder.output_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.embedder(x)\n",
        "        logits = self.fc_policy(hidden)\n",
        "        p = Categorical(logits=logits)\n",
        "        v = self.fc_value(hidden).reshape(-1)\n",
        "        return p, v"
      ],
      "metadata": {
        "id": "YSMLuDirskEw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### procgen vec_env wrappers"
      ],
      "metadata": {
        "id": "ygaathnjty-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "See https://github.com/openai/baselines/tree/master/baselines/common/vec_env\n",
        "\"\"\"\n",
        "\n",
        "class VecEnv(ABC):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "    Used to batch data from multiple copies of an environment, so that\n",
        "    each observation becomes an batch of observations, and expected action is a batch of actions to\n",
        "    be applied per-environment.\n",
        "    \"\"\"\n",
        "    closed = False\n",
        "    viewer = None\n",
        "\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array']\n",
        "    }\n",
        "\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a dict of observation arrays.\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "        Returns (obs, rews, dones, infos):\n",
        "         - obs: an array of observations, or a dict of\n",
        "                arrays of observations.\n",
        "         - rews: an array of rewards\n",
        "         - dones: an array of \"episode done\" booleans\n",
        "         - infos: a sequence of info objects\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close_extras(self):\n",
        "        \"\"\"\n",
        "        Clean up the  extra resources, beyond what's in this base class.\n",
        "        Only runs when not self.closed.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        if self.closed:\n",
        "            return\n",
        "        if self.viewer is not None:\n",
        "            self.viewer.close()\n",
        "        self.close_extras()\n",
        "        self.closed = True\n",
        "\n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Step the environments synchronously.\n",
        "        This is available for backwards compatibility.\n",
        "        \"\"\"\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        imgs = self.get_images()\n",
        "        bigimg = \"ARGHH\" #tile_images(imgs)\n",
        "        if mode == 'human':\n",
        "            self.get_viewer().imshow(bigimg)\n",
        "            return self.get_viewer().isopen\n",
        "        elif mode == 'rgb_array':\n",
        "            return bigimg\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def get_images(self):\n",
        "        \"\"\"\n",
        "        Return RGB images from each environment\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def unwrapped(self):\n",
        "        if isinstance(self, VecEnvWrapper):\n",
        "            return self.venv.unwrapped\n",
        "        else:\n",
        "            return self\n",
        "\n",
        "    def get_viewer(self):\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.SimpleImageViewer()\n",
        "        return self.viewer\n",
        "\n",
        "    \n",
        "class VecEnvWrapper(VecEnv):\n",
        "    \"\"\"\n",
        "    An environment wrapper that applies to an entire batch\n",
        "    of environments at once.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, venv, observation_space=None, action_space=None):\n",
        "        self.venv = venv\n",
        "        super().__init__(num_envs=venv.num_envs,\n",
        "                        observation_space=observation_space or venv.observation_space,\n",
        "                        action_space=action_space or venv.action_space)\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        self.venv.step_async(actions)\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_wait(self):\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        return self.venv.close()\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.venv.render(mode=mode)\n",
        "\n",
        "    def get_images(self):\n",
        "        return self.venv.get_images()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        if name.startswith('_'):\n",
        "            raise AttributeError(\"attempted to get missing private attribute '{}'\".format(name))\n",
        "        return getattr(self.venv, name)\n",
        "\n",
        "    \n",
        "class VecEnvObservationWrapper(VecEnvWrapper):\n",
        "    @abstractmethod\n",
        "    def process(self, obs):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.venv.reset()\n",
        "        return self.process(obs)\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rews, dones, infos = self.venv.step_wait()\n",
        "        return self.process(obs), rews, dones, infos\n",
        "\n",
        "        \n",
        "class VecExtractDictObs(VecEnvObservationWrapper):\n",
        "    def __init__(self, venv, key):\n",
        "        self.key = key\n",
        "        super().__init__(venv=venv,\n",
        "            observation_space=venv.observation_space.spaces[self.key])\n",
        "\n",
        "    def process(self, obs):\n",
        "        return obs[self.key]\n",
        "    \n",
        "    \n",
        "class RunningMeanStd(object):\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "        \n",
        "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class VecNormalize(VecEnvWrapper):\n",
        "    \"\"\"\n",
        "    A vectorized wrapper that normalizes the observations\n",
        "    and returns from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, venv, ob=True, ret=True, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n",
        "        VecEnvWrapper.__init__(self, venv)\n",
        "\n",
        "        self.ob_rms = RunningMeanStd(shape=self.observation_space.shape) if ob else None\n",
        "        self.ret_rms = RunningMeanStd(shape=()) if ret else None\n",
        "        \n",
        "        self.clipob = clipob\n",
        "        self.cliprew = cliprew\n",
        "        self.ret = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rews, news, infos = self.venv.step_wait()\n",
        "        for i in range(len(infos)):\n",
        "            infos[i]['env_reward'] = rews[i]\n",
        "        self.ret = self.ret * self.gamma + rews\n",
        "        obs = self._obfilt(obs)\n",
        "        if self.ret_rms:\n",
        "            self.ret_rms.update(self.ret)\n",
        "            rews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n",
        "        self.ret[news] = 0.\n",
        "        return obs, rews, news, infos\n",
        "\n",
        "    def _obfilt(self, obs):\n",
        "        if self.ob_rms:\n",
        "            self.ob_rms.update(obs)\n",
        "            obs = np.clip((obs - self.ob_rms.mean) / np.sqrt(self.ob_rms.var + self.epsilon), -self.clipob, self.clipob)\n",
        "            return obs\n",
        "        else:\n",
        "            return obs\n",
        "\n",
        "    def reset(self):\n",
        "        self.ret = np.zeros(self.num_envs)\n",
        "        obs = self.venv.reset()\n",
        "        return self._obfilt(obs)\n",
        "\n",
        "\n",
        "class TransposeFrame(VecEnvWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(venv=env)\n",
        "        obs_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(obs_shape[2], obs_shape[0], obs_shape[1]), dtype=np.float32)\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, reward, done, info = self.venv.step_wait()\n",
        "        return obs.transpose(0,3,1,2), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.venv.reset()\n",
        "        return obs.transpose(0,3,1,2)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(VecEnvWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(venv=env)\n",
        "        obs_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=obs_shape, dtype=np.float32)\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, reward, done, info = self.venv.step_wait()\n",
        "        return obs/255.0, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.venv.reset()\n",
        "        return obs/255.0"
      ],
      "metadata": {
        "id": "aw6KtRVet7cE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPO"
      ],
      "metadata": {
        "id": "HJOupbopuZVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 policy,\n",
        "                 logger,\n",
        "                 storage,\n",
        "                 device,\n",
        "                 n_steps,\n",
        "                 n_envs,\n",
        "                 epoch,\n",
        "                 mini_batch_per_epoch,\n",
        "                 mini_batch_size,\n",
        "                 gamma,\n",
        "                 lmbda,\n",
        "                 learning_rate,\n",
        "                 eps_clip,\n",
        "                 value_coef,\n",
        "                 entropy_coef):\n",
        "\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "        self.logger = logger\n",
        "        self.storage = storage\n",
        "        self.device = device\n",
        "        self.n_steps = n_steps\n",
        "        self.n_envs = n_envs\n",
        "        self.epoch = epoch\n",
        "        self.mini_batch_per_epoch = mini_batch_per_epoch\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.gamma = gamma\n",
        "        self.lmbda = lmbda\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate, eps=1e-5)\n",
        "        self.eps_clip = eps_clip\n",
        "        self.value_coef = value_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.t = 0\n",
        "\n",
        "    def predict(self, obs, done):\n",
        "        with torch.no_grad():\n",
        "            obs = torch.FloatTensor(obs).to(device=self.device)\n",
        "            dist, value = self.policy(obs)\n",
        "            act = dist.sample()\n",
        "            log_prob_act = dist.log_prob(act)\n",
        "\n",
        "        return act.cpu().numpy(), log_prob_act.cpu().numpy(), value.cpu().numpy()\n",
        "\n",
        "    def optimize(self):\n",
        "        pi_loss_list, value_loss_list, entropy_loss_list = [], [], []\n",
        "        self.policy.train()\n",
        "        for e in range(self.epoch):\n",
        "            generator = self.storage.fetch_train_generator(mini_batch_size=self.mini_batch_size)\n",
        "            for sample in generator:\n",
        "                obs_batch, act_batch, done_batch, \\\n",
        "                    old_log_prob_act_batch, old_value_batch, return_batch, adv_batch = sample\n",
        "                dist_batch, value_batch = self.policy(obs_batch)\n",
        "\n",
        "                # Clipped Surrogate Objective\n",
        "                log_prob_act_batch = dist_batch.log_prob(act_batch)\n",
        "                ratio = torch.exp(log_prob_act_batch - old_log_prob_act_batch)\n",
        "                surr1 = ratio * adv_batch\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.eps_clip, 1.0 + self.eps_clip) * adv_batch\n",
        "                pi_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # useful info to log\n",
        "                approx_kl = (old_log_prob_act_batch - log_prob_act_batch).mean().item()\n",
        "                clipped = ratio.gt(1+self.eps_clip) | ratio.lt(1-self.eps_clip)\n",
        "                clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
        "\n",
        "                # Clipped Bellman-Error\n",
        "                clipped_value_batch = old_value_batch + (value_batch - old_value_batch).clamp(-self.eps_clip, self.eps_clip)\n",
        "                v_surr1 = (value_batch - return_batch).pow(2)\n",
        "                v_surr2 = (clipped_value_batch - return_batch).pow(2)\n",
        "                value_loss = 0.5 * torch.max(v_surr1, v_surr2).mean()\n",
        "\n",
        "                # Policy Entropy\n",
        "                entropy_loss = dist_batch.entropy().mean()\n",
        "                loss = pi_loss + self.value_coef * value_loss - self.entropy_coef * entropy_loss\n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                pi_loss_list.append(pi_loss.item())\n",
        "                value_loss_list.append(value_loss.item())\n",
        "                entropy_loss_list.append(entropy_loss.item())\n",
        "\n",
        "        summary = {'loss/clipfrac': clipfrac,\n",
        "                   'loss/approxkl': approx_kl,\n",
        "                   'loss/policy_loss': np.mean(pi_loss_list),\n",
        "                   'loss/value_loss': np.mean(value_loss_list),\n",
        "                   'loss/policy_entropy': np.mean(entropy_loss_list)}\n",
        "        return summary\n",
        "\n",
        "    def train(self, num_timesteps):\n",
        "        obs = self.env.reset()\n",
        "        done = np.zeros(self.n_envs)\n",
        "\n",
        "        while self.t < num_timesteps:\n",
        "            # Run Policy\n",
        "            self.policy.eval()\n",
        "            for _ in range(self.n_steps):\n",
        "                act, log_prob_act, value = self.predict(obs, done)\n",
        "                next_obs, rew, done, info = self.env.step(act)\n",
        "                self.storage.store(obs, act, rew, done, info, log_prob_act, value)\n",
        "                obs = next_obs\n",
        "            _, _, last_val = self.predict(obs, done)\n",
        "            self.storage.store_last(obs, last_val)\n",
        "            # Compute advantage estimates\n",
        "            self.storage.compute_estimates(self.gamma, self.lmbda)\n",
        "\n",
        "            # Optimize policy & valueq\n",
        "            summary = self.optimize()\n",
        "            # Log the training-procedure\n",
        "            self.t += self.n_steps * self.n_envs\n",
        "            rew_batch, done_batch = self.storage.fetch_log_data()\n",
        "            self.logger.feed(rew_batch, done_batch)\n",
        "            self.logger.write_summary(summary)\n",
        "            self.logger.dump()\n",
        "        self.env.close()"
      ],
      "metadata": {
        "id": "hGdGgQ-tubPS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train"
      ],
      "metadata": {
        "id": "1fCCRDsmr59F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVdZYuFFrzvY"
      },
      "outputs": [],
      "source": [
        "env_name = 'starpilot'\n",
        "distribution_mode = 'easy'\n",
        "num_timesteps = 25000000\n",
        "seed = 3407\n",
        "\n",
        "set_global_seeds(seed)\n",
        "\n",
        "## HYPERPARAMETERS ##\n",
        "print('[LOADING HYPERPARAMETERS...]')\n",
        "hyperparameters = {\n",
        "    'n_envs': 64,\n",
        "    'n_steps': 256,\n",
        "    'epoch': 3,\n",
        "    'mini_batch_per_epoch': 8,\n",
        "    'mini_batch_size': 2048,\n",
        "    'gamma': 0.999,\n",
        "    'lmbda': 0.95,\n",
        "    'learning_rate': 0.0005,\n",
        "    'eps_clip': 0.2,\n",
        "    'value_coef': 0.5,\n",
        "    'entropy_coef': 0.01,\n",
        "}\n",
        "for key, value in hyperparameters.items():\n",
        "    print(key, ':', value)\n",
        "\n",
        "## DEVICE ##\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## ENVIRONMENT ##\n",
        "print('INITIALIZAING ENVIRONMENTS...')\n",
        "n_envs = hyperparameters.get('n_envs')\n",
        "env = ProcgenEnv(num_envs=n_envs,\n",
        "                    env_name=env_name,\n",
        "                    distribution_mode=distribution_mode)\n",
        "env = VecExtractDictObs(env, \"rgb\")\n",
        "env = VecNormalize(env, ob=False) # normalizing returns, but not the img frames.\n",
        "env = TransposeFrame(env)\n",
        "env = ScaledFloatFrame(env)\n",
        "\n",
        "## LOGGER ##\n",
        "print('INITIALIZAING LOGGER...')\n",
        "logdir = 'procgen/' + env_name + '/' + 'seed' + '_' + \\\n",
        "            str(seed) + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "logdir = os.path.join('logs', logdir)\n",
        "if not (os.path.exists(logdir)):\n",
        "    os.makedirs(logdir)\n",
        "logger = Logger(n_envs, logdir)\n",
        "\n",
        "## MODEL ##\n",
        "print('INTIALIZING MODEL...')\n",
        "observation_space = env.observation_space\n",
        "observation_shape = observation_space.shape\n",
        "in_channels = observation_shape[0]\n",
        "action_space = env.action_space\n",
        "assert isinstance(action_space, gym.spaces.Discrete)\n",
        "model = ImpalaModel(in_channels=in_channels)\n",
        "action_size = action_space.n\n",
        "policy = CategoricalPolicy(model, action_size).to(device)\n",
        "\n",
        "## STORAGE ##\n",
        "print('INITIALIZAING STORAGE...')\n",
        "n_steps = hyperparameters.get('n_steps')\n",
        "storage = Storage(observation_shape, n_steps, n_envs, device)\n",
        "\n",
        "## AGENT ##\n",
        "print('INTIALIZING AGENT...')\n",
        "agent = PPO(env, policy, logger, storage, device, **hyperparameters)\n",
        "\n",
        "## TRAINING ##\n",
        "print('START TRAINING...')\n",
        "agent.train(num_timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(logdir + '/log.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3_X9zNkIvdPU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "d009551f-3990-41a7-a234-3ef7ce1985d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   timesteps  wall_time  num_episodes  max_episode_rewards  \\\n",
              "0    16384.0  19.582341         175.0                 12.0   \n",
              "1    32768.0  31.448236         382.0                  6.0   \n",
              "2    49152.0  43.407000         579.0                  7.0   \n",
              "3    65536.0  55.559462         774.0                 14.0   \n",
              "4    81920.0  67.501515         958.0                 11.0   \n",
              "\n",
              "   mean_episode_rewards  min_episode_rewards  max_episode_len  \\\n",
              "0                 2.075                  0.0            186.0   \n",
              "1                 1.000                  0.0            137.0   \n",
              "2                 1.375                  0.0            177.0   \n",
              "3                 2.525                  0.0            241.0   \n",
              "4                 2.350                  0.0            160.0   \n",
              "\n",
              "   mean_episode_len  min_episode_len  \n",
              "0            82.525             23.0  \n",
              "1            70.725             19.0  \n",
              "2            79.075             18.0  \n",
              "3            90.300             39.0  \n",
              "4            87.000             31.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-435ae3d3-ea92-4b42-b1ec-1c9d25c2bed7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timesteps</th>\n",
              "      <th>wall_time</th>\n",
              "      <th>num_episodes</th>\n",
              "      <th>max_episode_rewards</th>\n",
              "      <th>mean_episode_rewards</th>\n",
              "      <th>min_episode_rewards</th>\n",
              "      <th>max_episode_len</th>\n",
              "      <th>mean_episode_len</th>\n",
              "      <th>min_episode_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16384.0</td>\n",
              "      <td>19.582341</td>\n",
              "      <td>175.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>82.525</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32768.0</td>\n",
              "      <td>31.448236</td>\n",
              "      <td>382.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>70.725</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49152.0</td>\n",
              "      <td>43.407000</td>\n",
              "      <td>579.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>79.075</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>65536.0</td>\n",
              "      <td>55.559462</td>\n",
              "      <td>774.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.525</td>\n",
              "      <td>0.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>90.300</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81920.0</td>\n",
              "      <td>67.501515</td>\n",
              "      <td>958.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.350</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>87.000</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-435ae3d3-ea92-4b42-b1ec-1c9d25c2bed7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-435ae3d3-ea92-4b42-b1ec-1c9d25c2bed7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-435ae3d3-ea92-4b42-b1ec-1c9d25c2bed7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df['timesteps'], df['mean_episode_rewards'], 'b', alpha=0.8);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T_GITSsYo3Jw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "3b4f1173-9530-4989-d38b-cc60071b0520"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dedgUxbXG3/MBogQFF0QUFRFjQuIagltCkGiixou7onkE44orXnJDDIm43bigEY1ruJFEjdddIyJqTNCQ64afCojBBXfFKCbKKiB8df+oqXRPTy/V2/T0zPt7nnmmu6en+vT29ulTVadEKQVCCCHlo61oAwghhCSDAk4IISWFAk4IISWFAk4IISWFAk4IISWlcz03tskmm6h+/frVc5OEEFJ6nn/++U+UUr28y+sq4P369UN7e3s9N0kIIaVHRN7xW84QCiGElBQKOCGElBQKOCGElBQKOCGElBQKOCGElBQKOCGElBQKOCGElBQKOCEtgFLA9OnA558XbQnJEgo4IS3Aiy8CEyYAv/pV0ZaQLKGAE9ICrFihvxctKtYOki0UcEIIKSkUcEIIKSkUcEJaAA5925xQwAkhpKRQwAlpIUSKtoBkCQWckJKwciUwYgTw0ktFW0IaBQo4ISXh1VeBBQuASZOKtoQ0ChRwQggpKRRwQggpKRRwQkoCmwJmT0cH8NRT5T22FHBCSMty223AWWcBM2cWbUkyKOCElAQ2Acye99/X32XNEUMBJ6QklPU1v5Exx7SsD0cKOCElI4nYUPzDSXJMFy0Cnnkme1vi0LnYzRNSTt58E+jbF1hnnfpt88QT05dRVk+zERk1Cvj4Y6C9vTgb6IETEpOlS4EjjwQuuKB+23zwwfpti9jx8cfhvy9cCIwbB6xalZ8NFHBCYmIGR3jxxfpt8/nn67etMvDuu8Dq1enLyTO0dOWVwIwZwJNP5rcNCjghMSl7xVfZWbYMOPRQ4KKLsiszj3NZj+uDAk5ITDo69Hc9BZwPC4eVK/X3rFnF2hGFOWd5evkUcEISQlEthraKapkHaRrq0Tpn7dr8tkMBJyQmRXjgxMEc9ywFPO25XL0aePTRaqE2ZY4fD5x3Xrryg6CAExITIxxtdbx7snpYzJuXTTlFYo57o7RtVwq47jrg5z+vbhfuPmfTp+ezbQo4IQkpygNPs91PP83OjqLIMrachQeulNMVf8kSZzkrMQlpQIpoheLeVqN4nvXmjTeq59euDV9/xQpg0CDg3nujy057Lv0eKhRwQhqQImLgrSrahhkzgKOOAh57zDkWUcfEeMW33ZavbUplG5ePAwWckJgU4YHX27NrNBYs0N9vvukciyixzOM4TZ6svXo3bgE3vPYa8Nlnznxe9SUUcNKQDBoE/OpXRVsRTj0rMevt2WVBR4c+jzfckE/5WcbAbZk8WX+7e4G6BdyUd8wx1e3Uu3ZNbmMY1pegiHQSkRdFZFplfhsReVZEFojInSJSx7Q+pBW4/faiLfCnCDEtYwjFHKff/95ZtnSpHkDhk0+Sl2uOxerVwBdfAM8+G54rxubYxfXWR492pu+7L3pbXbrEK9+WOD7EGADzXfOXAZiklBoA4FMAJ2RpGGkuHngAeP31oq3IBnOT1tMDbwQBf+89PfxYXNy2P/igLsMt6nEQqS7v7ruB00/XicVOOy3+wzXpcZ0715meOjW6aWNeWSutLkER6QvgBwB+W5kXAMMA3FNZ5WYAB+dhIGkOLroIOProoq3IBnOTLliQb6Iiv20WySGHaO/ZljCbs4pPf/65Mz1rVnUzviA7xo4F/vAH/9+SsHp1dNPGQgUcwFUAxgEwz7eNAXymlFpTmX8fwBZ+fxSRk0WkXUTaF5V13KIWZMUKYMiQWo9r+XLg29/Wr62titvLGzMm+/JPPRU42OMOlTEGnuVDx7Ys93EKEtWZM4GrrqpeluaBsmpVtIAXFkIRkQMBfKyUSpTQUik1WSk1SCk1qFevXkmKIAXw1ltaxG+8sXr5ggXa6/nNb4qxKy1DhwLHHZeujLy94eeec8Zq9NtmWVqh5HWc3BWIX3wRvF6c45RWwKdODV+nyBj4XgCGi8jbAO6ADp1cDaCniJgRffoC+CAXC0lDUYR4LF6s2/JmIQjLlqXvTl6EN+ze5osvOjnJk1CvcIzfdtI2wZw9GzjwQGf+t7+N3mYY3vXvuqu2zCi8LVL8WLAA+Mc/4pVrQ6SAK6V+ppTqq5TqB2AEgBlKqR8CeBzA4ZXVRgF4IHvzSCviFcgLLwQmTQJeeqkYewDgzju1HUBjtEK5+OLgddesAf75T/uy8sLvOKXd9pw52W7TO6blxIm1b51RLF9ut60sBqDwkqYe/acAxorIAuiY+E3ZmETKwNy5uq1rHuyzT/W8qZhas6Z23Xpx+eXOa3IRFYpeYfroo+B1TzwR+P73q5cVXQn61FPAv/7lzMf1wI393brZrWfDhx86NqV5s3SfG6XqGwePJeBKqSeUUgdWpt9USg1WSg1QSh2hlMpx5DdSb2xuhNdey2fbfi0JGglvDo4sxLGjA5gyJXicxThef1SIKE5ZL7wA3Hyz/fpu3D0mzzpLN/Mzy267Ldn1s9564b97xdTGviwJE/A8WqJwVHriC4cN88fbjRrQbwZpvauZM4Hrrw+Ok26zjV7HkDZ7ni0nn6y/R41Kv5133qledsIJwN/+Fq/MddcN/939cI3KmdI5B/VraA+ctB5eochC0CdM0ELobsNbZrKIbZq8GUEZ9rp3T78NQxae50MP6QRTYXg9/S++AH79a2c+KpugG1ub45TpXtfvulZK516Jw7RpwW84FHDSFJjk9u6R1h98MF4nkTA+/xw499zqmGueeAV8zZr4IhmV4TCJ6MYJJ8TlvPOAceP8f3v3XeCXv4wWUxub1qwBdttN9+Q182G49zkqVOTe/rnnAh942tHdcw9w5JE6jGTLvHnA00/7/0YBJ4WTV0jlggt0RdfzPr0N4orPww/rT9zWBElxC/jy5cDuu+t4dhKCuud7xcjmPAQJeN6taMaPB+6/H9hvv/D1bOxYtUo/CEzulKiHgt8+B10/7myBAPB//1c9P7+SOMTbJj8Kv2sYADp1ileODRRw4ks9Wi34idAppwTb0qjxeLeAL12qv20GEXAT5oE//bQzEnuSMoHq8xl1btNmgjQPoSw8cC9RZfrFwIMYObJ63t0p6O67ozvnBBGUfzyP65cCTnwxscqoiy7oJnn+eaepW9A6WV3Q77+f/wNnjz2Cf3Pf+KalQVgPQT+CBPy994Azz6xN/hTXA3djc6zSZIK0rRz0s++BB/R1M2+ervT0rhPVGSaOB+7FHZ657LLq38zoPo0GBZz4Mnu2/3KvcATdHKecAhx+ePg6pmnZhx/a2eQnWnPm6Lwhf/xj9P//+le77cTljjtqlyWt2LQ9vnEF3D2dJKNgHJJmaVyxQic9Gz1apzs47LD4D+aoVihhD4Cw+Lp3OLdGgQJOQknTCsWvlYm3gmfWLGDEiHg2LVmiQxRK6ZwtgF0vzbixTFv88kHH9cCDwkRp2g4HhRPOOSd5mTakTbPr7qQUV8DdoSa//5oshH4ECbhIPk0Os4ACTlJhc4O5vb+776793d0V2YYLLwQuuURXMvkJX5BNeVXeDR9eu+2sQihhx3f6dJ3bJarMqHKCCIrj77ln+P/iit0DD+gUr0OG6Hn320tQi44gTj7Z+X/cfQ47Z3lUQGYBBbxOFNkNPE/8bpJp04LXiXsj+JX/6af6e+XKeBWceQn4Vls50w89pL/jiIdSjnh4vdegcp57Tren/93vdFvlI46o7cEaVeH36qu6mdzy5brp3xNPVG/vkkv8/xcVHorrgV90UXUnJTe/+EW8soDkAh50j15wQbymhH6EjRiUBgp4HXjlFd20zNtMKYglS/TNVAb8bpLzz6+ev+46ZzqugLu9IqWARx5xbjT3TRVXwI88Eviv/4pni02511wT//833BBcaRwlQqtX63Ea33qrNjHTWWfph9wvf6lF0su112rxnzMHOPRQfTyyqAyOI+DettdZEDU6ThBho9dfcUVyewCgT590/w+CAl4HzPBLtqO3jBunb6Y04wbmRVyBAarjjnEFfNUqZxtPPqk9spdf1vM33uhUSsUV8DffjPeQDNvPtJ79I48403GPr4jjaXtDF6+8ogfeuP9+//oIv7LDQjK2xAmhHHRQ+u158Y5aX8Zc6rZQwOuI7cXz7rv6O0634LzI+oKPGx91v677JbkKE09v2+k0Qhsl4B0dOqSRhB49nGnv8Z44Mfr/5jrxeziGjYbuF35yp6lNeu7rOVaoH2UcvSgpFPAGxNyQRd8INsR9TU3igRtsBdzYNH16ddNBb2w+DmH7uXChDo+ZFAFp8J5zmyZ/YQJu04rFLdR//rMznbTlRdHXrVLALbfUdtRpRkogEa2HEaVGrfl2YxOjdZPGA3eLiyEqh8isWc50mnhr2H5Om6ZbUSQl7VuOW8DdyaKA8Ox9UefOff0VnU88Dh0d1ceBIRSSiqSdEczFtnhxeOa3ceOi805khfcGaG8Pb+p1ww3V83EfSlEtHqIEPIsczLNnJ+/IsXSp/1vCggX+dRwLFsQrX6T6ge9tphn0wLzzTmc6yGOOalPtx+rV0VkK88Zra5kePnGhgDcg3hv+Jz/RIh00TNaMGflVeHqF0Wvb2Wfrrt5BeAcoiOsBrYoYJiRKwAE9cECauOiJJybLbLhkCbD33rUPMUB3XjLtx922J+klaVrldHQAG2xQ/dsrr/j/5/LL4zd1tCGLMFJagmydORP43/+try15QwFvQIwHbi7EhQv1dx5j6vkxerT/8iVL4scVwyrRbDjhhPDfowT81lv10G9+3d2B+Pme42Cy3fmFfgDnfKZ9rTf/93tImXE8/YiTK8T2AdgI3q7XVqV0p6Q0Ya5GpUE7iDYXcbPpeVufmP/Z3hxps/e1t9duGwCGDYtflrdtclw6OsL3+667apf5rR+Uze/II6v3N0tszsPSpekezCJOCCRuHnJvc7swbFtEFV2BCfjvT1CnpLw5/PDonqtpaIDD3fwkFfArr9QJfuIK+I03At/8pv5v0QSN8RgH2/0eOVKHm/zYeOP4273ooupu8nGxsXvvvdOPLWqbvtWLsc9mWLMgkV+4UJ/jF17QbbqjQl71wHvc6zWwh5dx43TeGZMiIA/ogTcg5kZ85BGgb19nua2QGa90yJD03mUj1NrbpCEQAf7+d/0ZPLj29ySjoZhRYJJSrzzmSQXcZIH0e4vxEnQOzANu4EDdyifPkJRhyJDgrveA7rzUCNQjARY98AbD6w2tWuXEwIuIL65erW/eqJYFpvNRlDdpIxZegiri3AS1ZY7LsmW6DsDsTxr8BPyQQ3TX9qwQccpfuzbeNWKbxheIfogm7b6ehKgHYljcv57Uw/mhB14H4lzUXuG49Vb7ch57DPjZz+y3ZcO8eTqPSxTz5+ukTscck+32bXHfLH5DWtn2kjz7bN1s8NBD09vkl/vmvff0JyvcMfC4Ah6HKO++nrHvLMeWHDYsv2aP9Tgm9MDriM0TOey1K+rmvPTSePaQWt5+O7uyzPBqWZbpZd48J9dOnqkXosquV6ht0iRgo438f9tww/jlnXxyOnvCqMcxoYA3GGkEPO4Fs3Jldmlui24+1gixei9u0curcu+FF5wUA0oV54HPmaO/3Ym58uDb3w7+LclbU55eMkMoLUiYgEc197K5GB95BPjNb5zX+G98Q88THQbyjlSehltuya4sG/J8iNp693EH50hCkDAm6XOQp4AzhNJkpA2hzJ2ru0D7JXUC/FOBekXkF7+ojsH6xYvLSBbeTp5dwJP0BDXDxRl22CF8/Tw98DIMSBI3bUJbW75eMkMoLUiYgP/3f+su0EEDEfh1CPEmN2pW0ralBoIfjFmQRFiPOKJ6PsqjyzONaiOkNjZklfemUycKOMmAjg7Hw7E56XlWiiX14IqOgT/3XPL/Tpmiv/N8/f/Xv/wHVYhDVCKwiy/Or9PK/ffnU24Sgu6Rnj3jlRM1bmmS4dzcMIRScqZNqx1n0I/Ro+2a6uXNP/9p1ysviEbpQBGX66/X33k+hA4+WA9xloYiu6knab9fb7JsXgik7wJPD7zknH++3TiD7rEdbUQkL6E56aTkCX86OoDTT8/Wnnqy//7Ao4/mu40XX0z3/0bIM5KEq66Kt/6mm4b/HiSMSY5PmMj6vfHE6RZPAW8yRHRHnTABzkPA167VORmiejSm6X143nnJ/9sILFpUtAXR1KNrdh7YxKbd+5Y0g2VQuuUwwkTW74Hw85/bl502E6cNFPA6YAT3+ed1W9Wwob3iVkTZdId+7z3dvXz8+PD1GrEtNXHIYoSmLbZIX0ZcbEIb7sGNk16HWSdv8zvetraddRbwrW9la48fFPA68vrr+tv0nPPDRsDdHvh//Ef0+h99pL8p0Dq9Z1nJIoTyla+kLyMuNg8etwfut59DhgDXXKOng67jJC1l4nrgtmWOHNkglZgisq6IzBKROSLysohcUFm+jYg8KyILROROEclg8KrG509/Sp4sx1xgYfmf4wi4be8+E5sOyondSuT5EMu7IjoLDzwo3W6e+A315sXtpfsJ3+67A3vsEV5GFgL+1a+G22EjymmSqcXF5hmxCsAwpdROAHYGsJ+I7A7gMgCTlFIDAHwKIGLslOZg/Hhg6tR0ZYQ1X4pzES5eHG+7WXfFLyNZ7+M55zjTe+2VbdlesvDoNtkkfRk2uOO/IsA224Rv2+3U+O2n+83J7xwOGJCNg7L99uF2BF0/7qHaevRIb4ctkZeE0pg+fl0qHwVgGIB7KstvBnBwLhY2AV7hDBPwPDtjZDG4Aqlm772d6bxfmfNMvJQ1kyc70+a4hL1BuHsRe4/jBhtUL/MT0X32qRb5qKH4bLAV8ClTgC9/Of32kmB1yYlIJxGZDeBjAI8BeAPAZ0op08H2fQC+1SMicrKItItI+6IyVPVnxF//GvxbWg9cKR0+ybNDT7OSVGTdrQ923dWZdouSOwzw9a8n204YW2+dfZkAsPnmwAUXpCtjt92q591C16+f/g4TcHcMPMlbkkh180PbMrzruZ0t20rMHXe021YeWF3OSqm1SqmdAfQFMBiAdVWIUmqyUmqQUmpQr169EppZPsISRKWNga9YoW+4006Lb1erkzSE8p3vOALhzojnfiDssw9wwAG6niSPmzqvENf3vgekvTXDWpqsu67+tk3U5n1j9e6333GI+k8Q3v+55+OEUIoilj+ilPoMwOMA9gDQU0TMKekL4IOMbSs1YVntwrxsGw987drwIaXicuyxOpd4nuGbrPnOd+q7vU6dtNAB+iYfOlRPd+6seylOn65f9S+8UOerzqPNdl7iceqpegzVNLS1RYc5vOdsu+2cafe117u3/j766OT2JBVwbxmHHZas3Hph0wqll4j0rEyvB2BfAPOhhdxEnUYBSDmCYLk5/ng9iOlVVwGDBoXHm8NEOio/gyGswubss+3KMMyfD9xzT/R6jcJvfwuceGKy/ya9Ab2Z6375S11x1a0b0L9/be/BLFqMeMlKPPbZp3reJHVKU8EpokMx7nkvZ56pH3SGU0/VQ8wBzj2x7rqOsAeJqynbPFCDtmeDexubburUaQwcqL9/+tPqCsrSCTiAPgAeF5G5AJ4D8JhSahqAnwIYKyILAGwM4Kb8zGx85s7V6Uj/8IfodbMQ8DDMxdes7Lxzuja6Sejevfq/XbuGV1zFsa9eKQhM7D6oQ9cGG9Qu23tvuxwubW06U6bB7zi3tdU+6HbeWX9vvLH+Xmcd57/GKw86Z1lUHLo9/wceqB3tp62ttkWNYaON4mdAzJrIFz2l1FwAu/gsfxM6Hk4i8IYm8hbwsubMiENSDzdJE69evapvXJtUBnHss1k3i1QFZsg9P6EGgG23rR1VPk5loDuObvM/EV1n0KWLrjMwnm7Uf83v7vNglo0aBdx8s53NQ4f6l+HFfX7c67jfJoqiBW714vEKtpl/551aMcgicT4FPJgDDrDzmg44wJmeOLH6t6wFvF45TqK2c+65+uPFZn+7dIl/3Zmw1Pe+59imlCOSZtmWW1b/z0/Ahw937LBlwAA7AXfvl3u6c+fi89O0wK1ePH4C/swzuoLk4Yerf2tWD3zSpPRl7Lgj0N6up5MKeKdO1Xk3gvC7aeOEX+LYF1ZuVO/DOETZ1K2b/7GxEfAzz6zeD1sP3OB3zXbvrq8bm2vHG7+32f5669W+Hfv9z3aw5MGD82k+GkYD3urNh/ci6ehwXlW9GQIbxQPP+vVwr72q208nwS1AxvP50pecmP9WW0WXsWaNXUufsBYVNoIW5xy41/X25jQx2Swqz5I+9Nz7e+ed/vvWo0ftQ+/II+2bupr/uj1wpXSTTe9ADWHHIk6mzj32qG02aMR6F1fQeL317LZ9/fXA739vv/0soID7sGIF8NBD2ZXnFfA1a4Arr9TT7hpuoHE88Lijm9jgvfiTvHIbjBgp5RzfL30puowePewekmk98Div1u5yJ0508meHxd0fe8y+fEMSARep3va22+oWOF68rXREdKus448PLjsqgVWUGEc1AQxj2DBdCWrK6N9f/6dPH+C++4Irbhvt7bakGYbz5dJLtQe65ZbZdMjwCkZQE8B3381mbMcsLrKsL1SR2jLb2uK1PfcT8LVrnTK6dYsuo1u3+B641+6sPXBv6xaT8lUkWIjWX9++fINbwMeP10OwxbEtar2410xQ78s0bbhtPXBvSxy3LWFvciK6Qjnt8HhZQQH34ZNP9HdWJ8l7UQUJyKGHZrO9RhVwmx51YQR54AYbAQecvNHrrw8sXRq9fpLwRRxv11vxZpM7JIlN7uM3fLidgAO1bxN+IukVcBv7oioco9qBhxG1jqnI7t8fOOYYHe6xxSaFc71osBeCxsBciFmNxH3rrdXzSUYOiUNa8R0+PJ9XxbQC7hY0M93R4ZTjjlWGceyxOm7+0ENOO2QvfulN86rE9Aqk2U7YOUgbE7c9vyJ24SCbnpgGc2z9ylWqOh4eZJP5/Xe/SzbgsvsYjx0L9O0b73+NAgXch6gLqNGJI75+Y/zllfQ/S9ExN79bwN25nMPYcUfgllu0xx4Uwuna1anQKsoDdy/3y/VhXvXdKVBtiXON2DTNi+OBm/Lc5ZqH76hR8dqB77BDdTNDc5xEtCPiTvfrV0ZcKOAlwK+daZmIc3P6dZ+2+b87j4Ut3os/bosbvxCKW8C/8Y34NoW9ZZlOP6b8ONeFzTEcMEB/Bwl4167BgiGiE6Zdcw1w223Vv40e7UzvvrvuIeytLLdFpLbdfFAIxTaObR6+bg+8c2fdRNRd6Rl0nE3elqj8LRMmBI/A1GhCnBQKuA/mBrKtYGs0oTf221R0+XnbBx4Y/b/jjgM22yyWWTXETZ7lF0Jxk+Q8hNmQ5rzaeOCm+7g3lGAeKn6D4rrDRL16+bcT79PHmVZK97xM0+3ctO4xcWIbAQ/DzwP3I+j477qr7kexS03/cPtzFlfAx461D9HVEwq4D3E98Cya/uWBjYB/7Ws6m54bk/4zjE6dnLSqtpWHSbye2293pv088KTlGsI8cPfruN/yMGwE3Dw8vAJuhspbd12nR6gRq9tu8+8tGUQWzUGHDdP5Ws44Q8/b7H/YOYnqCWuuK/dgGV6i4vJZe9jHHAP87W/ZlpkFFHAf4nrgWY81mUcmuyBEahP42NDR4RwftwgGJUpKynbbOa/VfjHW73/feRPw81ijsDnHSUIotmmBAccTNcLWvbv+HjxYhwna250471ZbRfckFdE5yf/zP4Gf/Sx4vZEjgRtuiC6rrQ340Y/sH9RAeAjJXG9Bx3677fQ+f+1r9tsz5OWBNypsRuhDXA/cdnBhWwYOBF56yZlPmj7V5iLt1Kn6Zuvf3758cwN6O3qksceP3XfXw1aZmLFhxgwtKitX6sRE3t9tMPvQuzfw0UfVv3k98Dj2/+Mf0euYOoAuXbRnbYRts830W5FNz9IgNtoI+OEPw9exyTLoR1oP/PLLgUcftW/5kYSoc9VoHXKS0iS7kS3edJZ+zJ3rTGct4N62yTvvnEz8bG80t8cf1nMO0K/Tpmw/D1wk+Ma03YcrrtAtRcwo77vuqh9iP/pR9XobbKBfpbt3r04+FQcT1zTHwMR59903+PhlVedh3ty6dNEtSdzZ/Pr3T54oqUjv0nRCCnsD2XRT3ZQzDzttzs1DD9X3LTdPKOA+uFs4BOEWurwFHMivotTbfjdqpBt31jhjkzefxJQp1f8xmeLc9O4NjBlTu3zwYO1NT5kCXHutszxJDnATajAjvPgxcaJOxGS83b320q/v7iaJSTxwmxY2ZjCDsLeWJGQpjHGPuc29kyfGeXAPLjFxoh6YwRB2PZQNCrgPcUMoixdnu/203oG56Wxilm1t1duLqml3t5E3x8d9s/rF1M3vZqTwU0/VXtCxx9aWn5X49OypK/uefDK8o0fv3rrtsV+FZZqHpk1F8BFHAM89Z7duUSTtLVuUgB9yCHDjjfoNyjBsmD7WzQgFvMKaNY7XZBNCceMX7+zRI/kwZWlFrE8fnQXOJg2n1wP344oram1Tyv812c92cxx32EF7t0bIg+xJg+nUcvXV+rtrV7v83+54tCEoBm4j7LbjOeYRRsiyzLj5RooWcBE9pGGzVFJGQQGvMHSoM8Ze3J6Yfq1Q4rSL9ZLG89twQ73d44+vbg8chI2ADx3qTJuem9tv79jpTvjlV1bShFVJMG8TcY+9aQrqJ/amLNNm3qanalQbZ5ONMiuuuy7b8gx+12LYW1rWaShIOBTwCitXAkuW6Om4IRS/9bzpNeOQ5uK3eR2/8EJn2kbA3ey7rw5LbLut7syz+eZajExs3Ozzqac6/wkT8DvuqJ5PK+BJH35GwMM88CFDgKlTo+sJoujXzz+FQRp2282ZztL79Dt37ge6l6I98DCuu84/FW6ZoYB7uOcefy9i4UL9auZH0MWa9EYKu/hNy4w0uFtsJBFM09562221oPXs6VQamX0+4QTgkkv0dNj+DBigB5M1FXp5vfredZe2NQg/ATfTbohRbTwAABBzSURBVJvclWNRBLVLnzDBvoyi+cEPape1tenBuw87rPa3007TbzFpmkDmxW676X4DzQQF3MOllzqitmqV9lY/+QSYPTv4P2m7FnsJ88DdeS788G7z/POrvWFvytqsBNP0+jSpWgF7b2yLLZwu4Wk98G99S397c7z07x8uvkZs3aJ79dXASSelTxngZuzYbHLMh7HTTtmV5R0hyPCVr/h3EvrWt4CnnorX6Yckhx15fDCi9vjjwKxZOrzil3fBYJMfOQ5hghdV5ogR1fMmr4npceftKWlr44QJ4R1lDjkEePnl6hYocSqDTVzZz+OLw0kn6RYHcXuXXnaZ7lzizmy35ZbAKackt6WoirS0zeTOOw+44IJsbCH5Qg/cByNqc+bo77Vr9Q0exOWXB5eRhDAPPEwUTj9d52yIQ5ide+7p9MwcPtwZe9KPgw/WDzz3q3OcNsGbb65bqITlv7ChrS1ZaoDNNrNLZZqE7bf3bwvfqDTSgAUkHAq4D+YmNh10wirGwjps5BFCCRPcJJ5XWHm//nVtoqswvMmzGrlCq57ccIPdeJ1lJIuslCQ5FHAf4oyBGDbsWlIvPKkHvv/+9ttI0rswLhRwTTO3ST7jDGDatKKtaF1aTsCXLdOfMLw3XJCgDhoUPDq4e6CBuLgFz4xQHmSb7W9B5JkTwsSTs2g5U2ba2pw6lLxGOyKtScsJ+NChte1Y33mnen758ur5sDDJ9On+y9PkCDf5kAGnVYVp0tbWlk0b4np44FttpR9wtr0Smw33MR42TKd43XXXYm0izUXLCbiXV1+tbc86f371/FNPBf9/9Wr/5V98kSyEct99tVn3gGoxuPLK4AdHXNw27rlnNmW6MT1DWxHvfiepXC2KY49Nll+d1JeWF/CFC2uXvfWW/f+93rthzRp/4TrqqOp5vy7XO+xQu8w7Urm37KD2ukF4y5s1y8kfQsiYMbrHLWlsWl7A06Zp9YZbDEED0rpHED/mmOoc0GGECfjVV8cXX69tabr+E3/MueVxJXnR8gKeFzfd5H/jHnhgdY/AoDELO3eujpd6Y9bu0Edc7xvQPU532CE66RJJzg03ABdd1JiD4ZLmoGUF/KmngA8/zKfs4cN1Bxg/AW9rq+4t+atf+ZfxzDPA5MnOvFfAzfcGG9jZtNNO1Wlchw4Ffvc7eod5summ8Zp2EhKXlhXws86qjUdnTZQ4KqVfs70Z+fwwHre3YtS2ovSmm6pzohBCyk/k7S8iW4rI4yLydxF5WUTGVJZvJCKPicjrle8N8zc3W9yJl/LARsABnWPEnYMjrCxvqlt60IS0Ljb+2xoAP1ZKDQSwO4DTRWQggHMA/EUptR2Av1TmiYsgcTWdOtyxa9O2u0eP8LK8vRsp4MQwYwbw5z8XbQWpJ5HZCJVSHwL4sDK9VETmA9gCwEEAhlZWuxnAEwB+6lNEyxHUScbMf/WrwLPPVveCHDMGGDkyWMC9ZRgPPG36VdI82NaHkOYh1u0vIv0A7ALgWQC9K+IOAP8A4JtKSUROFpF2EWlftGhRClPzIa/R3oFacXULurcLe1sbsPHGwWV5PfCePXX8fNy49HYSQsqJtYCLSHcA9wI4Wym1xP2bUkoB8JVCpdRkpdQgpdSgXraNnkvKSSflV7a3/XeXLsDDD+su2oSQ1sRKwEWkC7R436aUuq+y+CMR6VP5vQ+Aj/MxMV9mzsyurEMO0UJ7xBF6Psv4NGPdhBAvNq1QBMBNAOYrpdxjaU8FMKoyPQrAA9mblz8PPWS/blQui0031V3STcY5bwgl7mALbuKMbkMIaQ1shlTbC8CxAF4SETMy5HgAlwK4S0ROAPAOgCPzMbFx+J//0e2pbRNJuePWzz6bbtveyktCCLFphfJ/AIJe4L+brTmNx4EH6oT1++8PbL118lYfaUMgDKEQQrywEVoI7e3A4MF62ni+K1fa/z/LJn777ae/m3VoLkJIfFpKwN94I/5/vJ5vHAHP0ms+80zgiSco4IQQh5YScHcSqbiEeeBBgwlnKeBtbUD37tmVRwgpPy0l4FlUAP74x7XDYt1xB/Dgg7XriuiWK+PHp98uIYR4sWmFQlxst51O8zpokLNs/fX1x4uIHgeREELyoGU88LffTva/oDDIvfcmNoUQQjKhZQT8gw+yLa/JswIQQkpAywh4Wrzxc7bLJoQUTcsIeNYjrjONKyGkaFpGht58M9n/1llHf3frVr2cHjghpGhaRsBtGTq0dn70aODss6uXU8AJIUXT1AK+fDnwzjvx/uM3aPCJJ9Z2omEIhRBSNE0tQ6NHA4cdFu8/tr0d6YETQoqmqQV8/vz4//nxj+3Wo4ATQoqmqQXccN990esYbJNFUcAJIUXTEgJ+8cVFW0AIIdnTEgKeJ/vuW7QFhJBWhcmsXHzve/HWnzGjtn04IYTUi5b0wE1I5etfB6ZOrV0+cCBw7rnR5WywAdCZj0BCSEG0pPyYCsjevYHNN6/9/ZZb6msPIYQkoSU9cNMJx52gip40IaRstKRsGQ/cCPhllwHbb1+cPYQQkoSWFvCODv393e8WZwshhCSFIRRCCCkpLSngW2yhv3feuVg7CCEkDS0XQrn4YmDAAOD++x0hJ4SQMtJyAr7uuvp7yy2LtYMQQtLStCGUuHnACSGkbDStgH/ySdEWEEJIvjStgJuxLAkhpFlpSgFfvRr47DP/39h0kBDSLDRlJeb48cATTxRtBSGE5EtTeuBh4r3ttnUzgxBCcqUpBTyMvn2LtoAQQrIhMoQiIlMAHAjgY6XU1yvLNgJwJ4B+AN4GcKRS6tP8zIxHjx7A4sV6+tprdd7uDz5g/JsQ0lzYeOC/B7CfZ9k5AP6ilNoOwF8q8w2DEW8A2GQTPUDDvvvGH3GHEEIamUgBV0rNBPAvz+KDANxcmb4ZwMEZ25WIZcuAKVOql623XjG2EEJI3iRthdJbKfVhZfofAHoHrSgiJwM4GQC22mqrhJuzY+jQ2mUbbpjrJgkhpDBSV2IqpRSAwOiyUmqyUmqQUmpQr1690m4uNvTACSHNSlIB/0hE+gBA5fvj7EzKhv32A2bOdAZvIISQZiOpgE8FMKoyPQrAA9mYkxxvC5MVK4Bu3YqxhRBC6kGkgIvI7QCeBrC9iLwvIicAuBTAviLyOoB9KvOFcvfd1fNjxhRjByGE1IvISkyl1NEBPzXUSJLt7dXzW29djB2EEFIvmqYn5hdfFG0BIYTUl6YR8DVriraAEELqS9MI+NNPO9NHHVWcHYQQUi+aQsCvv96Z/vKXgZ/8pDhbCCGkXpRewBcurO4+v/HGxdlCCCH1pPQCvnp19fwvflGMHYQQUm+aTsB7B2ZlIYSQ5qLpBJwQQloFCjghhJSU0gs4O/AQQlqV0gv4ggVFW0AIIcVQagFXCpg0qWgrCCGkGEot4C+9VLQFhBBSHKUW8GXLiraAEEKKI+mYmA3BqlXO9AEHAHvuWZwthBBSb0or4DNnVg/icOGFxdlCCCFFUFoBHzu2aAsIIaRYShkD945/SQghrUjpBHz5cuCb36xedt11xdhCCCFFUjoBf+ut2mW77VZ/OwghpGhKJeDLl7PnJSGEGEpTifnFF8B3vlO7/PHH628LIYQ0AqXxwF95pXbZhAnA+uvX3xZCCGkESiPgnTrVLuPgDYSQVqY0IZQrr3SmJ0wANtsMGDy4OHsIIaRoSiHgn3wCzJ7tzO+9N0MnhBBSihDKn/7kTI8cSfEmhBCgJAJ+yy3O9BlnFGcHIYQ0EqUQcBH9fc01QFspLCaEkPwphRwaAf/Sl4q1gxBCGolSCLgZuJgCTgghDqUQcOOBb7ZZsXYQQkgjUYpmhNdfDzz1FD1wQghxk8oDF5H9RORVEVkgIudkZZSXAQN080FCCCEOiQVcRDoBuA7A/gAGAjhaRAZmZRghhJBw0njggwEsUEq9qZRaDeAOAAdlYxYhhJAo0gj4FgDec82/X1lWhYicLCLtItK+aNGiFJsjhBDiJvdWKEqpyUqpQUqpQb169cp7c4QQ0jKkEfAPAGzpmu9bWUYIIaQOpBHw5wBsJyLbiMg6AEYAmJqNWYQQQqJI3A5cKbVGRM4A8CiATgCmKKVezswyQgghoaTqyKOUmg5geka2EEIIiYEopeq3MZFFAN5J+PdNAHySoTllgPvcGrTaPrfa/gLp93lrpVRNK5C6CngaRKRdKTWoaDvqCfe5NWi1fW61/QXy2+dSJLMihBBSCwWcEEJKSpkEfHLRBhQA97k1aLV9brX9BXLa59LEwAkhhFRTJg+cEEKICwo4IYSUlIYT8KhBIkSkq4jcWfn9WRHpV38rs8Nif48TkUUiMrvyObEIO7NERKaIyMciMi/gdxGRX1eOyVwR2bXeNmaNxT4PFZHFrvM8od42ZomIbCkij4vI30XkZREZ47NOU51ny33O9jwrpRrmA90l/w0A/QGsA2AOgIGedU4DcGNlegSAO4u2O+f9PQ7AtUXbmvF+DwGwK4B5Ab8fAOBhAAJgdwDPFm1zHfZ5KIBpRduZ4f72AbBrZXp9AK/5XNtNdZ4t9znT89xoHrjNIBEHAbi5Mn0PgO+KmGGPS0dLDoqhlJoJ4F8hqxwE4BaleQZATxHpUx/r8sFin5sKpdSHSqkXKtNLAcxH7XgBTXWeLfc5UxpNwG0Gifj3OkqpNQAWA9i4LtZlj9WgGAAOq7xi3iMiW/r83mzYHpdmYw8RmSMiD4vI14o2JisqYc5dADzr+alpz3PIPgMZnudGE3BSy4MA+imldgTwGJy3D9JcvACd72InANcA+GPB9mSCiHQHcC+As5VSS4q2px5E7HOm57nRBNxmkIh/ryMinQH0APDPuliXPZH7q5T6p1JqVWX2twC+USfbiqTlBgtRSi1RSi2rTE8H0EVENinYrFSISBdoIbtNKXWfzypNd56j9jnr89xoAm4zSMRUAKMq04cDmKEqtQMlJHJ/PTHB4dBxtWZnKoCRlVYKuwNYrJT6sGij8kRENjN1OSIyGPreLKtjgsq+3ARgvlLqyoDVmuo82+xz1uc5VT7wrFEBg0SIyIUA2pVSU6EP0K0isgC6UmhEcRanw3J/zxKR4QDWQO/vcYUZnBEicjt0bfwmIvI+gPMAdAEApdSN0DnmDwCwAMAKAD8qxtLssNjnwwGcKiJrAHwOYESJHRMA2AvAsQBeEpHZlWXjAWwFNO15ttnnTM8zu9ITQkhJabQQCiGEEEso4IQQUlIo4IQQUlIo4IQQUlIo4IQQkhNRScw8605yJbl6TUQ+i/wPW6EQQkg+iMgQAMugc758Pcb/zgSwi1Lq+LD16IETQkhO+CUxE5FtReQREXleRP4mIl/x+evRAG6PKr+hOvIQQkgLMBnAaKXU6yKyG4DrAQwzP4rI1gC2ATAjqiAKOCGE1IlKoqs9AdztyoLd1bPaCAD3KKXWRpVHASeEkPrRBuAzpdTOIeuMAHC6bWGEEELqQCW97FsicgTw72HldjK/V+LhGwJ42qY8CjghhOREJYnZ0wC2F5H3ReQEAD8EcIKIzAHwMqpH4RoB4A7bBFdsRkgIISWFHjghhJQUCjghhJQUCjghhJQUCjghhJQUCjghhJQUCjghhJQUCjghhJSU/wezFZkL/rXNSgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}