{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtJ1DeaZnSrGTCSdOBuyaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckkissane/deep_learning_curriculum/blob/scaling-laws/solutions/1_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a decoder-only transformer language model."
      ],
      "metadata": {
        "id": "jAfhoJND1hWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some first principle questions to answer:"
      ],
      "metadata": {
        "id": "lWZOApbm1lpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is different architecturally from the Transformer, vs a normal RNN, like an LSTM? (Specifically, how are recurrence and time managed?)"
      ],
      "metadata": {
        "id": "bdR9qOnm1pc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer:\n",
        "* Non sequential: sequences are processed as a whole using multi-headed attention layers, which allows for parallel computation\n",
        "* Positional encodings are used so that the transformer can capture sequential information\n",
        "\n",
        "RNN:\n",
        "* Sequential processing: sequences are processed one token at a time using recurrent layers, which is not parallelizable\n",
        "* No positional encoding: RNNs learn positional information based on the past hidden state. This can cause issues with long sequences, as we lose information from older inputs"
      ],
      "metadata": {
        "id": "vWKIL7ae2jUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention is defined as, Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V. What are the dimensions for Q, K, and V? Why do we use this setup? What other combinations could we do with (Q,K) that also output weights?"
      ],
      "metadata": {
        "id": "ApUznB8T3s-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimensions are:\n",
        "* Q: (seq_len, d_k)\n",
        "* K: (seq_len, d_k)\n",
        "* V: (seq_len, d_v)\n",
        "\n",
        "1. d_k represents the dimension of the vectors representing the queries / keys. \n",
        "2. d_v is the dimension of the vectors representing the values.\n",
        "3. Since there are query, key, and value vectors for each token in the sequence, it's natural to pack them into matrices for more efficient computation. That's why we have seq_len rows for each matrix. \n",
        "\n",
        "\n",
        "Other combinations we could do with (Q, K) that output weights:\n",
        "* Additive attention computes the compatibility function using a feed-forward network with a single hidden layer\n",
        "\n",
        "However, \"dot-product attention is\n",
        "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
        "matrix multiplication code.\""
      ],
      "metadata": {
        "id": "zkWNOD3l3vJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Are the dense layers different at each multi-head attention block? Why or why not?"
      ],
      "metadata": {
        "id": "X6ka7QQs3x-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes\n",
        "\n",
        "Here are some ideas why:\n",
        "* Intuitively, the point of stacking layers is so that each layer can transform the data independently of each other, resulting in a more expressive model\n",
        "* The W^Q, W^K, W^V layers learn representations for the query, key, and values. The model will likely benefit from the flexibility of learning different representations for each block\n",
        "* It's been empirically observed that [more learnable parameters lead to better performance](https://arxiv.org/abs/2001.08361)"
      ],
      "metadata": {
        "id": "w8hN7j_6331t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we have so many skip connections, especially connecting the input of an attention function to the output? Intuitively, what if we didn't?"
      ],
      "metadata": {
        "id": "qBOhnUJc4Daq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the [ResNet paper](https://arxiv.org/abs/1512.03385?context=cs), it was observed that some deep neural networks perform worse than their shallow counterparts. Adding skip connections empirically seemed to solve this issue. \n",
        "The intuition is that adding skip connections allows layers to learn the identity mapping more easily. \n",
        "\"To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.\"\n",
        "\n",
        "If we didn't include these skip connections, we might experience a degradation of performance for very deep transformer models due to vanishing / exploding gradient problems."
      ],
      "metadata": {
        "id": "Ee7aXo_X4IAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we'll actually implement the code. Make sure each of these is completely correct - it's very easy to get the small details wrong. Implement the positional embedding function first."
      ],
      "metadata": {
        "id": "hLeHrVXP4R-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch import einsum\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import re"
      ],
      "metadata": {
        "id": "7i3AgKB_uGjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(3407)"
      ],
      "metadata": {
        "id": "opvq8Npb7FNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I use learned encodings, rather than the fixed encodings used in Attention is All You Need\n",
        "# This is because learned encodings seem to be popular in decoder-only models, like GPT-2\n",
        "# plus, it's simpler to implement in pytorch\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_position_embeddings, hidden_size):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_position_embeddings, hidden_size)\n",
        "    \n",
        "    def forward(self, pos):\n",
        "        return self.pos_embedding(pos)\n",
        "\n",
        "# sanity check\n",
        "config = dict(hidden_size=32, max_position_embeddings=8, seq_len=5)\n",
        "pos_emb = PositionalEmbedding(config['max_position_embeddings'], config['hidden_size'])\n",
        "\n",
        "pos = torch.arange(config['seq_len'])\n",
        "print(f\"pos: {pos}\")\n",
        "pos_embeddings = pos_emb(pos)\n",
        "print(f\"pos_embeddings: {pos_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li3xH4VK4TTH",
        "outputId": "35bb7bb5-5b35-4157-fdc9-975cbdd8be1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos: tensor([0, 1, 2, 3, 4])\n",
            "pos_embeddings: torch.Size([5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Then implement the function which calculates attention, given (Q,K,V) as arguments."
      ],
      "metadata": {
        "id": "yHsNT2yJ47C2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    p_attn = scores.softmax(dim=-1)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "# sanity check\n",
        "test_q = torch.randn(1, 5, 64) # (batch_size, seq_len, d_k)\n",
        "test_k = torch.randn(1, 5, 64) # (batch_size, seq_len, d_k)\n",
        "test_v = torch.randn(1, 5, 64) # (batch_size, seq_len, d_v)\n",
        "\n",
        "out = attention(test_q, test_k, test_v)\n",
        "print(\"out shape:\", out[0].shape)\n",
        "print(f\"p_attn: {out[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg8LQkfN4fcY",
        "outputId": "cdad1d72-17ec-45f0-dbca-7b0a097d734e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out shape: torch.Size([1, 5, 64])\n",
            "p_attn: tensor([[[0.2474, 0.0444, 0.3023, 0.2104, 0.1955],\n",
            "         [0.3195, 0.4795, 0.0223, 0.0933, 0.0853],\n",
            "         [0.2972, 0.1110, 0.1023, 0.3373, 0.1522],\n",
            "         [0.6928, 0.1399, 0.0174, 0.1139, 0.0360],\n",
            "         [0.4379, 0.0779, 0.1950, 0.1982, 0.0909]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now implement the masking function."
      ],
      "metadata": {
        "id": "KKoZTHQQ5mEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_scores(attn_scores):\n",
        "    seq_len = attn_scores.shape[-2]\n",
        "    neg_inf = torch.tensor(-1e9).to(attn_scores.device)\n",
        "    q_ind = torch.arange(seq_len).unsqueeze(1)\n",
        "    k_ind = torch.arange(seq_len).unsqueeze(0)\n",
        "    mask = (q_ind < k_ind).to(attn_scores.device)\n",
        "    attn_scores = torch.where(mask, neg_inf, attn_scores)\n",
        "    return attn_scores\n",
        "\n",
        "#sanity check \n",
        "test_scores = torch.randn(1, 4, 4) # (batch_size, seq_len, seq_len)\n",
        "mask_scores(test_scores)"
      ],
      "metadata": {
        "id": "3Sx3vgYx5vO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea3f754-5ce2-4a37-cd6c-c1879c1c6020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.2899e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
              "         [-1.0423e+00, -1.8570e-01, -1.0000e+09, -1.0000e+09],\n",
              "         [ 1.1555e+00,  5.7537e-01, -3.6150e-01, -1.0000e+09],\n",
              "         [-1.4767e+00,  2.6147e-01,  1.4466e+00,  2.0954e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rewrite attn to always use mask for decoder-only model\n",
        "def masked_attention(query, key, value):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    scores = mask_scores(scores)\n",
        "    p_attn = scores.softmax(dim=-1)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "# sanity check\n",
        "test_q = torch.randn(1, 5, 64) # (batch_size, seq_len, d_k)\n",
        "test_k = torch.randn(1, 5, 64) # (batch_size, seq_len, d_k)\n",
        "test_v = torch.randn(1, 5, 64) # (batch_size, seq_len, d_v)\n",
        "\n",
        "out = masked_attention(test_q, test_k, test_v)\n",
        "print(\"out shape:\", out[0].shape)\n",
        "print(f\"p_attn: {out[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTLDc7FnMjUT",
        "outputId": "137f0b9c-c287-4666-bfd7-02f27cd6f6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out shape: torch.Size([1, 5, 64])\n",
            "p_attn: tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4210, 0.5790, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1711, 0.3003, 0.5287, 0.0000, 0.0000],\n",
            "         [0.5162, 0.1221, 0.1815, 0.1802, 0.0000],\n",
            "         [0.2538, 0.0574, 0.1897, 0.3095, 0.1897]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put it all together to form an entire attention block."
      ],
      "metadata": {
        "id": "zWVDk2c257Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model):\n",
        "        super().__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.output_proj = nn.Linear(d_model, d_model)\n",
        "        self.attn = None\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query = self.q_proj(query).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key = self.k_proj(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.v_proj(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = masked_attention(query, key, value)\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = (\n",
        "            x.transpose(1, 2)\n",
        "            .contiguous()\n",
        "            .view(batch_size, -1, self.h * self.d_k)\n",
        "        )\n",
        "        out = self.output_proj(x)\n",
        "        return out\n",
        "\n",
        "#sanity check\n",
        "test_query = torch.randn(1, 5, 24) # (batch_size, seq_len, d_model)\n",
        "test_key = torch.randn(1, 5, 24) # (batch_size, seq_len, d_model)\n",
        "test_value = torch.randn(1, 5, 24) # (batch_size, seq_len, d_model)\n",
        "\n",
        "multi_head_attn = MaskedMultiHeadedAttention(h=4, d_model=24)\n",
        "out = multi_head_attn(test_query, test_key, test_value)\n",
        "print(f\"out: {out.shape}\")"
      ],
      "metadata": {
        "id": "INLwC3hgQR8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f0bf09-c8f6-4fd9-d456-6f4a1379c186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out: torch.Size([1, 5, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finish the whole architecture."
      ],
      "metadata": {
        "id": "lGpUVDkS6Inf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        layer_norm_epsilon: float,\n",
        "        dropout: float,\n",
        "        num_heads: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.attn = MaskedMultiHeadedAttention(num_heads, hidden_size)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.linear1 = nn.Linear(hidden_size, hidden_size * 4)\n",
        "        self.linear2 = nn.Linear(hidden_size * 4, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x))\n",
        "        x = x + self.dropout(self.linear2(F.gelu(self.linear1(self.ln2(x)))))\n",
        "        return x\n",
        "\n",
        "#sanity check\n",
        "test_input = torch.randn(1, 5, 24) # (batch_size, seq_len, d_model)\n",
        "dec_block = DecoderBlock(hidden_size=24, layer_norm_epsilon=1e-4, dropout=0.1, num_heads=4)\n",
        "out = dec_block(test_input)\n",
        "print(f\"out: {out.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw0J_R-b9_UA",
        "outputId": "74698c74-513e-4d56-d588-e88990077e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out: torch.Size([1, 5, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        num_heads,\n",
        "        vocab_size,\n",
        "        hidden_size,\n",
        "        max_position_embeddings,\n",
        "        dropout,\n",
        "        layer_norm_epsilon\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.pos_embedding = nn.Embedding(max_position_embeddings, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                DecoderBlock(hidden_size, layer_norm_epsilon, dropout, num_heads)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "\n",
        "        print(f\"number of parameters: {sum(p.numel() for p in self.parameters())}\")\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        pos = torch.arange(seq_len).to(input_ids.device)\n",
        "        enc = self.dropout(self.token_embedding(input_ids) + self.pos_embedding(pos))\n",
        "        enc = self.blocks(enc)\n",
        "        enc = self.ln(enc)\n",
        "        logits = torch.einsum(\"bnl, vl -> bnv\", enc, self.token_embedding.weight)\n",
        "        return logits\n",
        "\n",
        "# sanity check\n",
        "config = dict(num_layers=2, num_heads=4, vocab_size=100, hidden_size=64,\n",
        "                max_position_embeddings=32, dropout=0.0, layer_norm_epsilon=1e-4)\n",
        "x = torch.randint(0, config['vocab_size'], (1, 5))\n",
        "\n",
        "dec_only_transformer = DecoderOnlyTransformer(**config)\n",
        "output = dec_only_transformer(x)\n",
        "print(f\"output: {output.shape}\")"
      ],
      "metadata": {
        "id": "N10t9I5u6G8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb40b6f-92c0-4f66-904e-2ef7edf39788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 108544\n",
            "output: torch.Size([1, 5, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To check you have the attention mask set up correctly, train your model on a toy task, such as reversing a random sequence of tokens. The model should be able to predict the second sequence, but not the first."
      ],
      "metadata": {
        "id": "2_8cMOU-6uT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Reverses sequences up to some number of digits in the inputs. Recall\n",
        "    that all GPT cares about are sequences of integers, and completing them according to\n",
        "    patterns in the data. Therefore, we have to somehow encode reversals\n",
        "    as a sequence of integers.\n",
        "    \n",
        "    As a few examples, for a 3 digit sequence:\n",
        "    - reverse([1,2,3]) = [3, 2, 1] becomes the sequence [1, 2, 3, 3, 2, 1]\n",
        "    - reverse([6, 8, 8]) = [8, 8, 6] becomes the sequence [6, 8, 8, 8, 8, 6]\n",
        "    etc.\n",
        "    \n",
        "    We will also only train GPT on the final n-digits because the first\n",
        "    (n-1)-digits are always assumed to be given. So when we give GPT an exam later,\n",
        "    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
        "    to reverse [0, 6, 3, 9], and hope that the model completes the integer sequence with [9, 3, 6, 0]\n",
        "    in 4 sequential steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, ndigit):\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        self.block_size = 2 * ndigit - 1\n",
        "        \n",
        "        self.size = 10**self.ndigit # total number of possible combinations\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp = torch.randint(self.vocab_size, size=(self.ndigit,), dtype=torch.long)\n",
        "        sol = torch.flip(inp,(-1,))\n",
        "        cat = torch.cat((inp, sol), dim=0)\n",
        "        x = cat[:-1].clone()\n",
        "        y = cat[1:].clone()\n",
        "        y[: self.ndigit - 1] = -100\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "H2EukbZKx5gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataset for e.g. 4-digit sequence reversals\n",
        "ndigit = 4\n",
        "train_dataset = ReverseDataset(ndigit=ndigit)"
      ],
      "metadata": {
        "id": "jzpYT76FyQle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHKmBwwMyVlO",
        "outputId": "51dabc98-d6a5-49c4-86da-bb2df6c96183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([7, 5, 4, 6, 6, 4, 5]),\n",
              " tensor([-100, -100, -100,    6,    4,    5,    7]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=512//2\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "BpQz8fYcyZcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUv71S_cyfJm",
        "outputId": "c85a73d0-b82c-42d8-8ffe-b69984584e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderOnlyTransformer(\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    hidden_size=128,\n",
        "    max_position_embeddings=train_dataset.block_size,\n",
        "    dropout=0.1,\n",
        "    layer_norm_epsilon=1e-5,\n",
        ").to(device).train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22FgaaJgyima",
        "outputId": "5667f75c-9a21-4dd7-8b2f-3d397c2407bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 398976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=6e-4)"
      ],
      "metadata": {
        "id": "ppuswQ2hyqMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 10\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXwq7biiy2c_",
        "outputId": "47778b88-e437-4d4a-ca32-7f91351b130c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 0 iter 39: train loss 3.07595: 100%|██████████| 40/40 [00:04<00:00,  8.18it/s]\n",
            "epoch 1 iter 39: train loss 2.44274: 100%|██████████| 40/40 [00:01<00:00, 35.21it/s]\n",
            "epoch 2 iter 39: train loss 1.97242: 100%|██████████| 40/40 [00:01<00:00, 33.08it/s]\n",
            "epoch 3 iter 39: train loss 1.38969: 100%|██████████| 40/40 [00:01<00:00, 32.54it/s]\n",
            "epoch 4 iter 39: train loss 0.01131: 100%|██████████| 40/40 [00:01<00:00, 33.51it/s]\n",
            "epoch 5 iter 39: train loss 0.00005: 100%|██████████| 40/40 [00:01<00:00, 33.42it/s]\n",
            "epoch 6 iter 39: train loss 0.00082: 100%|██████████| 40/40 [00:01<00:00, 33.53it/s]\n",
            "epoch 7 iter 39: train loss 0.00013: 100%|██████████| 40/40 [00:01<00:00, 33.09it/s]\n",
            "epoch 8 iter 39: train loss 0.00311: 100%|██████████| 40/40 [00:01<00:00, 33.31it/s]\n",
            "epoch 9 iter 39: train loss 0.00018: 100%|██████████| 40/40 [00:01<00:00, 34.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, block_size):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits = model(x_cond)\n",
        "        # pluck the logits at the final step\n",
        "        logits = logits[:, -1, :]\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # take the most likely\n",
        "        _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "eDnvMVQizCwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test: reverse [1, 2, 3, 4] -> [4, 3, 2, 1]\n",
        "inp = torch.tensor([[1, 2, 3, 4]]).to(device)\n",
        "y = sample(model, inp, 4, train_dataset.block_size)[0]\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQ0OwM4zLvx",
        "outputId": "96e7cf5c-b91c-4142-a407-2955374c1b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4, 4, 3, 2, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it shouldn't be able to predict the first sequence\n",
        "inp = torch.tensor([[1]]).to(device)\n",
        "y = sample(model, inp, 7, train_dataset.block_size)[0]\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D-CBErvzY4f",
        "outputId": "b6e37eb9-27ce-4c31-aa12-df6a3913b69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, train your model on the [complete works of William Shakespeare](https://www.gutenberg.org/files/100/100-0.txt). Tokenize the corpus by splitting at word boundaries (re.split(r\"\\b\", ...))."
      ],
      "metadata": {
        "id": "oZzN4Nsvkt8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you'll need to upload this file to your colab session\n",
        "text = open('100-0.txt', 'r').read()"
      ],
      "metadata": {
        "id": "5o2T2Fb3Knet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    arrange data and targets so that the first i elements of x\n",
        "    will be asked to predict the i-th element of y. Notice that\n",
        "    the eventual language model will actually make block_size\n",
        "    individual predictions at the same time based on this data,\n",
        "    so we are being clever and amortizing the cost of the forward\n",
        "    pass of the network. So for example if block_size is 4, then\n",
        "    we could e.g. sample a chunk of text \"w1 w2 w3 w4 w5\", the integers in\n",
        "    x will correspond to \"w1 w2 w3 w4\" and in y will be \"w2 w3 w4 w5\". This will\n",
        "    then actually \"multitask\" 4 separate examples at the same time\n",
        "    in the language model:\n",
        "    - given just \"w1\", please predict \"w2\" as next\n",
        "    - given \"w1 w2\" please predict \"w3\" next\n",
        "    - given \"w1 w2 w3\" predict \"w4\" next\n",
        "    - given \"w1 w2 w3 w4\" predict \"w5\" next\n",
        "    \"\"\"\n",
        "    def __init__(self, data, block_size):\n",
        "        words = re.split(r\"\\b\", data)\n",
        "        vocab = sorted(list(set(words)))\n",
        "        data_size, vocab_size = len(words), len(vocab)\n",
        "        print('data has %d words, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = {word: i for i, word in enumerate(vocab)}\n",
        "        self.itos = {i: word for i, word in enumerate(vocab)}\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = words\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every word to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "UUxTSOxqLkY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "train_dataset = WordDataset(text, block_size) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbrebM87LyS1",
        "outputId": "98210275-bd06-4743-d08f-98c0afd2324f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1987763 words, 34541 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "1uNjn_KeL6Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzzUC8HOMB3y",
        "outputId": "32388e13-3de5-4175-a22a-1795c5386815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderOnlyTransformer(\n",
        "    num_layers=8,\n",
        "    num_heads=8,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    hidden_size=512,\n",
        "    max_position_embeddings=train_dataset.block_size,\n",
        "    dropout=0.1,\n",
        "    layer_norm_epsilon=1e-5\n",
        ").to(device).train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8XUXDGTMEwM",
        "outputId": "ea594bd8-c6ff-4010-e2a7-cf5eeaca52a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 42970624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=6e-4)"
      ],
      "metadata": {
        "id": "VG_jyNmUMQlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 1\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_jks0WlMY13",
        "outputId": "acd535fe-0b4a-4b6e-e578-3e38c4b9ff5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 0 iter 31056: train loss 0.81490: 100%|██████████| 31057/31057 [3:23:02<00:00,  2.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out"
      ],
      "metadata": {
        "id": "hFgbF_ckMlT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "nsdA5WvFMpQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \" O God, O God! \"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in re.split(r\"\\b\", context)], dtype=torch.long)[None,...].to(device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usqxe28CMvNT",
        "outputId": "3bd2a675-fb35-48ee-f4cf-4495260a68a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " O God, O God! O this, O villain!\n",
            "O momentary churl!—duck, what bones are!\n",
            "Within my brows! How! ’Tis good she.\n",
            "How dost thou mean a woman’s negligence,\n",
            "To take her life? She’s dead again;\n",
            "Who, witty, so much, so much to do her good,\n",
            "Lies senseless offences death. Hie her therefore!\n",
            "Who is’t I say that she did say I?\n",
            "\n",
            "SECOND GENTLEMAN.\n",
            "I was sorry “Thomas heir,\n",
            "Will hold away her, let them be patient.\n",
            "The subject is already, not with a general touch;\n",
            "But the worst is free that did the feast.\n",
            "\n",
            "FIRST MURDERER.\n",
            "Ay, and you shall put upon the marriage-bed.\n",
            "\n",
            "HAMLET.\n",
            "Take my part, or I mean to think where it is.\n",
            "Imagine it so,—and so farewell yours,—\n",
            "For she’s a false thing, the academes of cream,\n",
            "That ’scuse on curtsies, guarded; but as a dove.\n",
            "I wonder in every cabin I shall not\n",
            "Outgo it so much conceit. I tell him plainly,\n",
            "For that’s true enough to disgrace, I will ease\n",
            "With full three thousand crowns; yet I hold one too,\n",
            "And yet by degrees, methinks I love mine honour,\n",
            "A father’s love, I will not break my mercy:\n",
            "I must my life before I shall pawn you\n",
            "To understand the worst that he shall feel distinction\n",
            "Of his own daughter, and his daughter’s relief.\n",
            "What, dwarf for lord? I say again;\n",
            "And with such powers smear’d I from my foot\n",
            "Advanced the very riping of mine on foot.\n",
            "I see the bottom of a hopeful dream,\n",
            "Whom I behold a tomb ’tween I from the banks\n",
            "Of mine own proper sword.\n",
            "\n",
            "YORK.\n",
            "My rightful brain I know,\n",
            "Which art a right commands me,\n",
            "A right valiant gentleman;\n",
            "And now my sovereign would take her to be\n",
            "As well as I can utter.\n",
            "\n",
            "IACHIMO.\n",
            "I grant she is, I know.\n",
            "She does usually gives the truth I had the overture I\n",
            "the virtue I hate her.\n",
            "\n",
            "POSTHUMUS.\n",
            "I will not, sir. You must not believe it.\n",
            "\n",
            "IACHIMO.\n",
            "I do not think I am suspected. First,\n",
            "Forbear your peril of difference,\n",
            "With all swift humbleness.\n",
            "\n",
            "IMOGEN.\n",
            "But I do.\n",
            "Most sure, the ignorant that I have sworn to do\n",
            "Will I search in nothing but I am bound to you\n",
            "For mercy wife’s. But now you, sir, the gods\n",
            "Will not sit along in to give you weapons?\n",
            "You understand not what you will, come paragon more than you.\n",
            "Your hands are so full.\n",
            "\n",
            "CLOTEN.\n",
            "You that do well to know what you are,\n",
            "And do it till you dare come to court,\n",
            "I’ll leave you for a lieutenant; but sure\n",
            "You think of late she and that is so tender\n",
            "Now chide up what request it contains.\n",
            "\n",
            "CLOTEN.\n",
            "Thou knowest true tribute, I could as from going.\n",
            "\n",
            "GUIDERIUS.\n",
            "There’s no art as mad as mine is no greater\n",
            "than they; for I gave them too much given to choose the stone and\n",
            "making the commodity of a butt brings us their leaves.\n",
            "\n",
            "CHAMBERLAIN.\n",
            "A crown twice could have well have spoke. Nothing but heart hath\n",
            "been with some care of him, that is one of the wicked. Therefore, beware I\n",
            "will find a piece of work. Let all the rest of justice hold me further.\n",
            "Diana shall I be gone.\n",
            "\n",
            " [_Exeunt._]\n",
            "\n",
            "\n",
            "\n",
            "ACT II\n",
            "\n",
            "SCENE I. Before the Tower.\n",
            "\n",
            " Enter Gower, before the monument of Marina at Tarsus.\n",
            "\n",
            "GOWER.\n",
            "Now comes Polonius in the Capulets.\n",
            "[_To Florizel] Pindarus, chance thy will,\n",
            "the foul fiend of the dark, art translated?\n",
            "We’ll hear you of that again comes; and we thank.\n",
            "[Tis for the misery of speed, now\n",
            "The match of your own daughters won,\n",
            "And homeward we meet again.\n",
            "\n",
            "KING.\n",
            "Alas, what then?\n",
            "Comes this a true sea of repentance you\n",
            "So now as doth import a sea of France\n",
            "And bear the lawful rage of your father’s court\n",
            "Breed terms to us again much the spur of it.\n",
            "\n",
            "BOLINGBROKE.\n",
            "I pray you, let us in the manner of his blood.\n",
            "\n",
            "KING RICHARD.\n",
            "He is no crescent. Have I not slain?\n",
            "\n",
            "AUMERLE.\n",
            "My liege. I speak to thee, gentle Percy:\n",
            "My spirits are no great; yet I know thy voice.\n",
            "And yet no other hath a power to pawn\n",
            "Mine honour didst live in them, but myself\n",
            "Must as half is fetch down. A conqueror.\n",
            "This gentleman is nothing, and, this is so:\n",
            "For dangers he is given so only to steal\n",
            "When majesty with tempest led his ship.\n",
            "He is a man indeed, and best indeed\n",
            "A comet at him.\n",
            "When that is return’d with Menelaus now,\n",
            "Take with the rest of care, with one life’s sense,\n",
            "I will have no mind in the repine.\n",
            "And I’ll give them him my destruction home.\n",
            "\n",
            " [_Exeunt._]\n",
            "\n",
            "\n",
            "\n",
            "ACT IV\n",
            "\n",
            " Enter Gower.\n",
            "\n",
            "GOWER.\n",
            "Now sleep yslaked hath the rouse;\n",
            "His benefit can throw at sea.\n",
            "I have sent for him to a cur,\n",
            "I found the eagle-lugg’d words;\n",
            "His sign of his bed-found temper’d fear\n",
            "Of murderous ships and life.\n",
            "I dare not think thou lov’st; for I protest\n",
            "Is most because Priam’s service, and ’tis horn,\n",
            "And I will live a servant.\n",
            "\n",
            "PROSPERO.\n",
            "Let order be\n",
            "What thou dost play:—I will show it on:\n",
            "We will both our drift, and they walk,\n",
            "Their cunning quarrels, their poor titles shall be.\n",
            "\n",
            " [_Exit._]\n",
            "\n",
            "SCENE II. Britain. Another room in Cymbeline’s palace.\n",
            "\n",
            " Enter Cymbeline, Queen, Cloten, Cloten and Lords; Pisanio all,\n",
            " Officers and Attendants.\n",
            "\n",
            "CYMBELINE.\n",
            "Stand by my side, you whom the gods have made\n",
            "Preservers of your sons and brothers obey. [_Knocking._] Knock, knock! Who’s there?\n",
            "A spirit, come you in some sort.\n",
            "\n",
            "[_To Rosencrantz and others._] How now, my women! What have you requir’d?\n",
            "How chance go to work?\n",
            "\n",
            "BANQUO.\n",
            "By heaven, your father.\n",
            "\n",
            "HELENA.\n",
            "Be overthrown yet, with the determination of all size.\n",
            "I took you a ring, and \n"
          ]
        }
      ]
    }
  ]
}