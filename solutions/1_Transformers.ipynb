{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckkissane/deep_learning_curriculum/blob/master/solutions/1_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAfhoJND1hWw"
      },
      "source": [
        "Implement a decoder-only transformer language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWZOApbm1lpO"
      },
      "source": [
        "Here are some first principle questions to answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdR9qOnm1pc-"
      },
      "source": [
        "## What is different architecturally from the Transformer, vs a normal RNN, like an LSTM? (Specifically, how are recurrence and time managed?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWKIL7ae2jUt"
      },
      "source": [
        "Transformer:\n",
        "* Non sequential: sequences are processed as a whole using multi-headed attention layers, which allows for parallel computation\n",
        "* Positional encodings are used so that the transformer can capture sequential information\n",
        "\n",
        "RNN:\n",
        "* Sequential processing: sequences are processed one token at a time using recurrent layers, which is not parallelizable\n",
        "* No positional encoding: RNNs learn positional information based on the past hidden state. This can cause issues with long sequences, as we lose information from older inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApUznB8T3s-W"
      },
      "source": [
        "## Attention is defined as, Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V. What are the dimensions for Q, K, and V? Why do we use this setup? What other combinations could we do with (Q,K) that also output weights?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkWNOD3l3vJj"
      },
      "source": [
        "The dimensions are:\n",
        "* Q: (seq_len, d_k)\n",
        "* K: (seq_len, d_k)\n",
        "* V: (seq_len, d_v)\n",
        "\n",
        "1. d_k represents the dimension of the vectors representing the queries / keys. \n",
        "2. d_v is the dimension of the vectors representing the values.\n",
        "3. Since there are query, key, and value vectors for each token in the sequence, it's natural to pack them into matrices for more efficient computation. That's why we have seq_len rows for each matrix. \n",
        "\n",
        "\n",
        "Other combinations we could do with (Q, K) that output weights:\n",
        "* Additive attention computes the compatibility function using a feed-forward network with a single hidden layer\n",
        "\n",
        "However, \"dot-product attention is\n",
        "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
        "matrix multiplication code.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ka7QQs3x-w"
      },
      "source": [
        "## Are the dense layers different at each multi-head attention block? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8hN7j_6331t"
      },
      "source": [
        "Yes\n",
        "\n",
        "Here are some ideas why:\n",
        "* Intuitively, the point of stacking layers is so that each layer can transform the data independently of each other, resulting in a more expressive model\n",
        "* The W^Q, W^K, W^V layers learn representations for the query, key, and values. The model will likely benefit from the flexibility of learning different representations for each block\n",
        "* It's been empirically observed that [more learnable parameters lead to better performance](https://arxiv.org/abs/2001.08361)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBOhnUJc4Daq"
      },
      "source": [
        "## Why do we have so many skip connections, especially connecting the input of an attention function to the output? Intuitively, what if we didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee7aXo_X4IAc"
      },
      "source": [
        "In the [ResNet paper](https://arxiv.org/abs/1512.03385?context=cs), it was observed that some deep neural networks perform worse than their shallow counterparts. Adding skip connections empirically seemed to solve this issue. \n",
        "The intuition is that adding skip connections allows layers to learn the identity mapping more easily. \n",
        "\"To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.\"\n",
        "\n",
        "If we didn't include these skip connections, we might experience a degradation of performance for very deep transformer models due to vanishing / exploding gradient problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLeHrVXP4R-h"
      },
      "source": [
        "## Now we'll actually implement the code. Make sure each of these is completely correct - it's very easy to get the small details wrong. Implement the positional embedding function first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7i3AgKB_uGjD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch import einsum\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Li3xH4VK4TTH"
      },
      "outputs": [],
      "source": [
        "# I use learned encodings, rather than the fixed encodings used in Attention is All You Need\n",
        "# This is because learned encodings seem to be popular in decoder-only models, like GPT-2\n",
        "# plus, it's simpler to implement in pytorch\n",
        "# Later, I won't use this class. I just use the nn.Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_position_embeddings, hidden_size):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_position_embeddings, hidden_size)\n",
        "    \n",
        "    def forward(self, pos):\n",
        "        return self.pos_embedding(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHsNT2yJ47C2"
      },
      "source": [
        "## Then implement the function which calculates attention, given (Q,K,V) as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jg8LQkfN4fcY"
      },
      "outputs": [],
      "source": [
        "def attention(q, k, v):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        - q: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "        - k: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "        - v: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "    Returns:\n",
        "        - out: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "    \"\"\"\n",
        "    headsize = q.shape[-1]\n",
        "    attn_scores = q.matmul(k.transpose(-1, -2)) / math.sqrt(headsize)\n",
        "    attn_scores = attn_scores.softmax(dim=-1)\n",
        "    out = attn_scores.matmul(v)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKoZTHQQ5mEI"
      },
      "source": [
        "## Now implement the masking function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3Sx3vgYx5vO_"
      },
      "outputs": [],
      "source": [
        "def mask_scores(attn_scores):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        scores: torch.tensor of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "    Returns:\n",
        "        out: torch.tensor of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    seq_len = attn_scores.shape[-2]\n",
        "    neg_inf = torch.tensor(-1e9).to(attn_scores.device)\n",
        "    q_ind = torch.arange(seq_len).unsqueeze(1)\n",
        "    k_ind = torch.arange(seq_len).unsqueeze(0)\n",
        "    mask = (q_ind < k_ind).to(attn_scores.device)\n",
        "    attn_scores = torch.where(mask, neg_inf, attn_scores)\n",
        "    return attn_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oTLDc7FnMjUT"
      },
      "outputs": [],
      "source": [
        "def masked_attn(q, k, v):\n",
        "    \"\"\"\n",
        "    in:\n",
        "        - q: torch.tensor of shape (batch_size, num_heads, seq_len, headsize)\n",
        "        - k: torch.tensor of shape (batch_size, num_heads, seq_len, headsize)\n",
        "        - v: torch.tensor of shape (batch_size, num_heads, seq_len, headsize)\n",
        "    out:\n",
        "        - out: torch.tensorof shape (batch_size, num_heads, seq_len, headsize)\n",
        "    \"\"\"\n",
        "    headsize = q.shape[-1]\n",
        "    attn_scores = q.matmul(k.transpose(-1, -2)) / math.sqrt(headsize)\n",
        "    attn_scores = mask_scores(attn_scores)\n",
        "    attn_scores = attn_scores.softmax(dim=-1)\n",
        "    out = attn_scores.matmul(v)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWVDk2c257Xx"
      },
      "source": [
        "## Put it all together to form an entire attention block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "INLwC3hgQR8I"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadedAttn(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_size):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        assert hidden_size % num_heads == 0\n",
        "        self.headsize = hidden_size // self.num_heads\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        in:\n",
        "            - x: torch.tensor of shape (batch_size, seq_len, hidden_size)\n",
        "        out:\n",
        "            - out: torch.tensor of shape (batch_size, seq_len, hidden_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.headsize).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.headsize).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.headsize).transpose(1, 2)\n",
        "        out = masked_attn(q, k, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        out = self.out_proj(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGpUVDkS6Inf"
      },
      "source": [
        "## Finish the whole architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Cw0J_R-b9_UA"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_size, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = MaskedMultiHeadedAttn(num_heads, hidden_size)\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "        self.lin1 = nn.Linear(hidden_size, hidden_size * 4)\n",
        "        self.lin2 = nn.Linear(hidden_size * 4, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        in:\n",
        "            - x : torch.tensor of shape (batch_size, seq_len, emb_dim)\n",
        "        out:\n",
        "            - out: torch.tensor of shape (batch_size, seq_len, emb_dim)\n",
        "        \"\"\"\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.dropout(self.lin2(F.gelu(self.lin1(self.ln2(x)))))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N10t9I5u6G8P"
      },
      "outputs": [],
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, max_pos_embeddings, num_heads, hidden_size, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.pos_embedding = nn.Embedding(max_pos_embeddings, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                DecoderBlock(num_heads, hidden_size, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(hidden_size)\n",
        "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        in: \n",
        "            - input_ids : torch.tensor of shape (batch_size, seq_len)\n",
        "        out:\n",
        "            - logits: torch.tensor of shape (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        pos = torch.arange(seq_len).to(input_ids.device)\n",
        "        tok_emb = self.token_embedding(input_ids)\n",
        "        pos_emb = self.pos_embedding(pos)\n",
        "        x = self.token_embedding(input_ids) + self.pos_embedding(pos)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        out = self.lm_head(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_8cMOU-6uT4"
      },
      "source": [
        "## To check you have the attention mask set up correctly, train your model on a toy task, such as reversing a random sequence of tokens. The model should be able to predict the second sequence, but not the first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H2EukbZKx5gR"
      },
      "outputs": [],
      "source": [
        "class ReverseDataset(Dataset):\n",
        "    def __init__(self, ndigit):\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        self.size = 10**self.ndigit # total number of possible combinations\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.randint(self.vocab_size, size=(self.ndigit,), dtype=torch.long)\n",
        "        y = torch.flip(x,(-1,))\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzpYT76FyQle",
        "outputId": "d9c6e43e-6a59-42d7-bf36-a5e10374116b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([4, 2, 1, 5, 3, 0]), tensor([0, 3, 5, 1, 2, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# create a dataset for e.g. 6-digit sequence reversals\n",
        "ndigit = 6\n",
        "train_dataset = ReverseDataset(ndigit=ndigit)\n",
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpQz8fYcyZcz",
        "outputId": "d950f90b-dfae-4a8b-ded5-b0a562520675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2048\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXwq7biiy2c_",
        "outputId": "8a832bd9-3f7b-459a-e317-1c1e71acd8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 0 iter 488: train loss 1.15268: 100%|██████████| 489/489 [00:25<00:00, 19.02it/s]\n"
          ]
        }
      ],
      "source": [
        "model = DecoderOnlyTransformer(\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    hidden_size=128,\n",
        "    max_pos_embeddings=train_dataset.ndigit,\n",
        "    dropout=0.1,\n",
        ").to(device).train()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=6e-4)\n",
        "\n",
        "max_epochs = 1\n",
        "for epoch in range(max_epochs):\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQ0OwM4zLvx",
        "outputId": "1f89275d-9089-4d56-cc82-8c6ffab8c560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: tensor([[5, 1, 5, 3, 2, 1]], device='cuda:0')\n",
            "answer: tensor([[6, 5, 4, 3, 2, 1]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# test: notice the first half of predictions are wrong, but the second half are correct\n",
        "inp = torch.tensor([[1, 2, 3, 4, 5, 6]]).to(device)\n",
        "logits = model(inp)\n",
        "print(\"prediction:\", logits.argmax(dim=-1))\n",
        "print(\"answer:\", torch.flip(inp, (-1,)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZzN4Nsvkt8W"
      },
      "source": [
        "## Finally, train your model on the [complete works of William Shakespeare](https://www.gutenberg.org/files/100/100-0.txt). Tokenize the corpus by splitting at word boundaries (re.split(r\"\\b\", ...))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5o2T2Fb3Knet"
      },
      "outputs": [],
      "source": [
        "# you'll need to upload this file to your colab session\n",
        "text = open('100-0.txt', 'r').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UUxTSOxqLkY7"
      },
      "outputs": [],
      "source": [
        "class WordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    arrange data and targets so that the first i elements of x\n",
        "    will be asked to predict the i-th element of y. Notice that\n",
        "    the eventual language model will actually make block_size\n",
        "    individual predictions at the same time based on this data,\n",
        "    so we are being clever and amortizing the cost of the forward\n",
        "    pass of the network. So for example if block_size is 4, then\n",
        "    we could e.g. sample a chunk of text \"w1 w2 w3 w4 w5\", the integers in\n",
        "    x will correspond to \"w1 w2 w3 w4\" and in y will be \"w2 w3 w4 w5\". This will\n",
        "    then actually \"multitask\" 4 separate examples at the same time\n",
        "    in the language model:\n",
        "    - given just \"w1\", please predict \"w2\" as next\n",
        "    - given \"w1 w2\" please predict \"w3\" next\n",
        "    - given \"w1 w2 w3\" predict \"w4\" next\n",
        "    - given \"w1 w2 w3 w4\" predict \"w5\" next\n",
        "    \"\"\"\n",
        "    def __init__(self, data, block_size):\n",
        "        words = re.split(r\"\\b\", data)\n",
        "        vocab = sorted(list(set(words)))\n",
        "        data_size, vocab_size = len(words), len(vocab)\n",
        "        print('data has %d words, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = {word: i for i, word in enumerate(vocab)}\n",
        "        self.itos = {i: word for i, word in enumerate(vocab)}\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = words\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every word to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbrebM87LyS1",
        "outputId": "c510b0b8-f2f4-4491-b248-a1a1b09ab262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1987763 words, 34541 unique.\n"
          ]
        }
      ],
      "source": [
        "block_size = 128\n",
        "train_dataset = WordDataset(text, block_size) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1uNjn_KeL6Z1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37fd76fb-9f12-4124-fed2-c2626161251a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: this can take hours. I stopped when loss was around 0.3"
      ],
      "metadata": {
        "id": "okz-X-H07a9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_jks0WlMY13"
      },
      "outputs": [],
      "source": [
        "model = DecoderOnlyTransformer(\n",
        "    num_layers=8,\n",
        "    num_heads=8,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    hidden_size=512,\n",
        "    max_pos_embeddings=train_dataset.block_size,\n",
        "    dropout=0.1,\n",
        ").to(device).train()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=6e-4)\n",
        "\n",
        "max_epochs = 1\n",
        "for epoch in range(max_epochs):\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hFgbF_ckMlT2"
      },
      "outputs": [],
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nsdA5WvFMpQs"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usqxe28CMvNT",
        "outputId": "e214d5e6-cc20-4fb0-a537-e597123ec38b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " O God, O God! Marry, a surgeon!\n",
            "\n",
            "BERTRAM.\n",
            "It is too true, good my lord; it is too much to do to make them good.\n",
            "\n",
            "LAFEW.\n",
            "I ne’er drank a woman and a knave to both your pardons.\n",
            "I am sure you have in this age to make a stand of that thou art\n",
            "but I shall lose distinction in the knowledge of thee. I am glad of\n",
            "ever I did hate thee worse than thou was born. I shall fall\n",
            "after thee. Come, fellow, look to my tent.\n",
            "\n",
            "LUCIUS.\n",
            "It is in Rome; I can tell thee what, lad,\n",
            "if it be not to think of thine. I bring thee boy\n",
            "And what thou think’st I will predominate thee in the more.\n",
            "\n",
            " Enter Cloten.\n",
            "\n",
            "KENT.\n",
            "Where’s Peaseblossom?\n",
            "\n",
            "ALEXAS.\n",
            "Thy lord of Rome. You have of the best quarrels,\n",
            "I have not seen the day, but I will dew this.\n",
            "\n",
            " Enter a doctor of ordnance.\n",
            "\n",
            "THIRD SOLDIER.\n",
            "How now, masters?\n",
            "What news, in thy name? I shall live long\n",
            "If ever I thy English coffin, return.\n",
            "Good Enobarbus, make haste after me\n",
            "To murder thee so far.\n",
            "\n",
            "FIRST MURDERER.\n",
            "O, I am fear’d of patience!\n",
            "\n",
            "THIRD MURDERER.\n",
            "All that which had the speed of Ross be transform’d,\n",
            "I’d to say it, and to this kind maid\n",
            "Our dearest choice, our brother, and our theme\n",
            "Say was it before us?\n",
            "\n",
            "BANQUO.\n",
            "Like the slave of Antioch.\n",
            "Fare thee well, boy.\n",
            "\n",
            " [_Exeunt._]\n",
            "\n",
            "SCENE VI. The wood. Before the cave \n"
          ]
        }
      ],
      "source": [
        "context = \" O God, O God! \"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in re.split(r\"\\b\", context)], dtype=torch.long)[None,...].to(device)\n",
        "y = sample(model, x, 500, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMCwMhUHMrMDZMcm2o5UfTq",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}