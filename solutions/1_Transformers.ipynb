{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckkissane/deep_learning_curriculum/blob/master/solutions/1_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAfhoJND1hWw"
      },
      "source": [
        "Implement a decoder-only transformer language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWZOApbm1lpO"
      },
      "source": [
        "Here are some first principle questions to answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdR9qOnm1pc-"
      },
      "source": [
        "## What is different architecturally from the Transformer, vs a normal RNN, like an LSTM? (Specifically, how are recurrence and time managed?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWKIL7ae2jUt"
      },
      "source": [
        "Transformer:\n",
        "* Non sequential: sequences are processed as a whole using multi-headed attention layers, which allows for parallel computation\n",
        "* Positional encodings are used so that the transformer can capture sequential information\n",
        "\n",
        "RNN:\n",
        "* Sequential processing: sequences are processed one token at a time using recurrent layers, which is not parallelizable\n",
        "* No positional encoding: RNNs learn positional information based on the past hidden state. This can cause issues with long sequences, as we lose information from older inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApUznB8T3s-W"
      },
      "source": [
        "## Attention is defined as, Attention(Q,K,V) = softmax(QK^T/sqrt(d_k))V. What are the dimensions for Q, K, and V? Why do we use this setup? What other combinations could we do with (Q,K) that also output weights?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkWNOD3l3vJj"
      },
      "source": [
        "The dimensions are:\n",
        "* Q: (seq_len, d_k)\n",
        "* K: (seq_len, d_k)\n",
        "* V: (seq_len, d_v)\n",
        "\n",
        "1. d_k represents the dimension of the vectors representing the queries / keys. \n",
        "2. d_v is the dimension of the vectors representing the values.\n",
        "3. Since there are query, key, and value vectors for each token in the sequence, it's natural to pack them into matrices for more efficient computation. That's why we have seq_len rows for each matrix. \n",
        "\n",
        "\n",
        "Other combinations we could do with (Q, K) that output weights:\n",
        "* Additive attention computes the compatibility function using a feed-forward network with a single hidden layer\n",
        "\n",
        "However, \"dot-product attention is\n",
        "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
        "matrix multiplication code.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ka7QQs3x-w"
      },
      "source": [
        "## Are the dense layers different at each multi-head attention block? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8hN7j_6331t"
      },
      "source": [
        "Yes\n",
        "\n",
        "Here are some ideas why:\n",
        "* Intuitively, the point of stacking layers is so that each layer can transform the data independently of each other, resulting in a more expressive model\n",
        "* The W^Q, W^K, W^V layers learn representations for the query, key, and values. The model will likely benefit from the flexibility of learning different representations for each block\n",
        "* It's been empirically observed that [more parameters lead to better performance](https://arxiv.org/abs/2001.08361)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBOhnUJc4Daq"
      },
      "source": [
        "## Why do we have so many skip connections, especially connecting the input of an attention function to the output? Intuitively, what if we didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee7aXo_X4IAc"
      },
      "source": [
        "In the [ResNet paper](https://arxiv.org/abs/1512.03385?context=cs), it was observed that some deep neural networks perform worse than their shallow counterparts. Adding skip connections empirically seemed to solve this issue. \n",
        "The intuition is that adding skip connections allows layers to learn the identity mapping more easily. \n",
        "\"To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.\"\n",
        "\n",
        "If we didn't include these skip connections, we might experience a degradation of performance for very deep transformer models due to vanishing / exploding gradient problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLeHrVXP4R-h"
      },
      "source": [
        "## Now we'll actually implement the code. Make sure each of these is completely correct - it's very easy to get the small details wrong. Implement the positional embedding function first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7i3AgKB_uGjD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch import einsum\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Li3xH4VK4TTH"
      },
      "outputs": [],
      "source": [
        "# I use learned encodings, rather than the fixed encodings used in Attention is All You Need\n",
        "# This is because learned encodings seem to be popular in decoder-only models, like GPT-2\n",
        "# plus, it's simpler to implement in pytorch\n",
        "# Later, I won't use this class. I just use the nn.Embedding\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_position_embeddings, hidden_size):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_position_embeddings, hidden_size)\n",
        "    \n",
        "    def forward(self, pos):\n",
        "        return self.pos_embedding(pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHsNT2yJ47C2"
      },
      "source": [
        "## Then implement the function which calculates attention, given (Q,K,V) as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jg8LQkfN4fcY"
      },
      "outputs": [],
      "source": [
        "def attention(q, k, v):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        - q: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "        - k: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "        - v: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "    Returns:\n",
        "        - out: torch.tensor(batch_size, num_heads, seq_len, headsize)\n",
        "    \"\"\"\n",
        "    headsize = q.shape[-1]\n",
        "    attn_scores = q.matmul(k.transpose(-1, -2)) / math.sqrt(headsize)\n",
        "    attn_scores = attn_scores.softmax(dim=-1)\n",
        "    out = attn_scores.matmul(v)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKoZTHQQ5mEI"
      },
      "source": [
        "## Now implement the masking function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3Sx3vgYx5vO_"
      },
      "outputs": [],
      "source": [
        "def mask_scores(attn_scores):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        attn_scores: torch.tensor of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "    Returns:\n",
        "        out: torch.tensor of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    seq_len = attn_scores.shape[-2]\n",
        "    neg_inf = torch.tensor(-1e9).to(attn_scores.device)\n",
        "    q_ind = torch.arange(seq_len).unsqueeze(1)\n",
        "    k_ind = torch.arange(seq_len).unsqueeze(0)\n",
        "    mask = (q_ind < k_ind).to(attn_scores.device)\n",
        "    attn_scores = torch.where(mask, neg_inf, attn_scores)\n",
        "    return attn_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oTLDc7FnMjUT"
      },
      "outputs": [],
      "source": [
        "def masked_attn(q, k, v):\n",
        "    \"\"\"\n",
        "    in:\n",
        "        - q: torch.tensor of shape (batch_size, num_heads, seq_len, headsize)\n",
        "        - k: torch.tensor of shape (batch_size, num_heads, seq_len, headsize)\n",
        "        - v: torch.tensor of shape (batch_size, num_heads, seq_len, headsize)\n",
        "    out:\n",
        "        - out: torch.tensor of shape (batch_size, num_heads, seq_len, headsize)\n",
        "    \"\"\"\n",
        "    headsize = q.shape[-1]\n",
        "    attn_scores = q.matmul(k.transpose(-1, -2)) / math.sqrt(headsize)\n",
        "    attn_scores = mask_scores(attn_scores)\n",
        "    attn_scores = attn_scores.softmax(dim=-1)\n",
        "    out = attn_scores.matmul(v)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWVDk2c257Xx"
      },
      "source": [
        "## Put it all together to form an entire attention block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "INLwC3hgQR8I"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadedAttn(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_size):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        assert hidden_size % num_heads == 0\n",
        "        self.headsize = hidden_size // self.num_heads\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        in:\n",
        "            - x: torch.tensor of shape (batch_size, seq_len, hidden_size)\n",
        "        out:\n",
        "            - out: torch.tensor of shape (batch_size, seq_len, hidden_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.headsize).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.headsize).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.headsize).transpose(1, 2)\n",
        "        out = masked_attn(q, k, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        out = self.out_proj(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGpUVDkS6Inf"
      },
      "source": [
        "## Finish the whole architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Cw0J_R-b9_UA"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, num_heads, hidden_size, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = MaskedMultiHeadedAttn(num_heads, hidden_size)\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "        self.lin1 = nn.Linear(hidden_size, hidden_size * 4)\n",
        "        self.lin2 = nn.Linear(hidden_size * 4, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        in:\n",
        "            - x : torch.tensor of shape (batch_size, seq_len, emb_dim)\n",
        "        out:\n",
        "            - out: torch.tensor of shape (batch_size, seq_len, emb_dim)\n",
        "        \"\"\"\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.dropout(self.lin2(F.gelu(self.lin1(self.ln2(x)))))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N10t9I5u6G8P"
      },
      "outputs": [],
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, max_pos_embeddings, num_heads, hidden_size, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.pos_embedding = nn.Embedding(max_pos_embeddings, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                DecoderBlock(num_heads, hidden_size, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(hidden_size)\n",
        "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        in: \n",
        "            - input_ids : torch.tensor of shape (batch_size, seq_len)\n",
        "        out:\n",
        "            - logits: torch.tensor of shape (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        pos = torch.arange(seq_len).to(input_ids.device)\n",
        "        x = self.dropout(self.token_embedding(input_ids) + self.pos_embedding(pos))\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        out = self.lm_head(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_8cMOU-6uT4"
      },
      "source": [
        "## To check you have the attention mask set up correctly, train your model on a toy task, such as reversing a random sequence of tokens. The model should be able to predict the second sequence, but not the first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H2EukbZKx5gR"
      },
      "outputs": [],
      "source": [
        "class ReverseDataset(Dataset):\n",
        "    def __init__(self, ndigit):\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        self.size = 10**self.ndigit # total number of possible combinations\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.randint(self.vocab_size, size=(self.ndigit,), dtype=torch.long)\n",
        "        y = torch.flip(x,(-1,))\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzpYT76FyQle",
        "outputId": "c7b0bd95-9b41-4759-a09b-9ba9190d5561"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3, 6, 4, 7, 7, 7]), tensor([7, 7, 7, 4, 6, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# create a dataset for e.g. 6-digit sequence reversals\n",
        "ndigit = 6\n",
        "train_dataset = ReverseDataset(ndigit=ndigit)\n",
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpQz8fYcyZcz",
        "outputId": "525d6542-1ec8-45ba-bb06-29f81e202e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2048\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXwq7biiy2c_",
        "outputId": "13b90acd-eda0-4121-8899-ee838902d3ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 0 iter 488: train loss 1.15316: 100%|██████████| 489/489 [00:25<00:00, 19.29it/s]\n"
          ]
        }
      ],
      "source": [
        "model = DecoderOnlyTransformer(\n",
        "    num_layers=2,\n",
        "    num_heads=4,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    hidden_size=128,\n",
        "    max_pos_embeddings=train_dataset.ndigit,\n",
        "    dropout=0.1,\n",
        ").to(device).train()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=6e-4)\n",
        "\n",
        "max_epochs = 1\n",
        "for epoch in range(max_epochs):\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQ0OwM4zLvx",
        "outputId": "24adb4fb-cb38-4213-9f96-de46ef00d8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: tensor([[4, 4, 9, 3, 2, 1]], device='cuda:0')\n",
            "answer: tensor([[6, 5, 4, 3, 2, 1]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# test: notice the first half of predictions are wrong, but the second half are correct\n",
        "inp = torch.tensor([[1, 2, 3, 4, 5, 6]]).to(device)\n",
        "logits = model(inp)\n",
        "print(\"prediction:\", logits.argmax(dim=-1))\n",
        "print(\"answer:\", torch.flip(inp, (-1,)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZzN4Nsvkt8W"
      },
      "source": [
        "## Finally, train your model on the [complete works of William Shakespeare](https://www.gutenberg.org/files/100/100-0.txt). Tokenize the corpus by splitting at word boundaries (re.split(r\"\\b\", ...))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5o2T2Fb3Knet"
      },
      "outputs": [],
      "source": [
        "# you'll need to upload this file to your colab session\n",
        "text = open('100-0.txt', 'r').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UUxTSOxqLkY7"
      },
      "outputs": [],
      "source": [
        "class WordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    arrange data and targets so that the first i elements of x\n",
        "    will be asked to predict the i-th element of y. Notice that\n",
        "    the eventual language model will actually make block_size\n",
        "    individual predictions at the same time based on this data,\n",
        "    so we are being clever and amortizing the cost of the forward\n",
        "    pass of the network. So for example if block_size is 4, then\n",
        "    we could e.g. sample a chunk of text \"w1 w2 w3 w4 w5\", the integers in\n",
        "    x will correspond to \"w1 w2 w3 w4\" and in y will be \"w2 w3 w4 w5\". This will\n",
        "    then actually \"multitask\" 4 separate examples at the same time\n",
        "    in the language model:\n",
        "    - given just \"w1\", please predict \"w2\" as next\n",
        "    - given \"w1 w2\" please predict \"w3\" next\n",
        "    - given \"w1 w2 w3\" predict \"w4\" next\n",
        "    - given \"w1 w2 w3 w4\" predict \"w5\" next\n",
        "    \"\"\"\n",
        "    def __init__(self, data, block_size):\n",
        "        words = re.split(r\"\\b\", data)\n",
        "        vocab = sorted(list(set(words)))\n",
        "        data_size, vocab_size = len(words), len(vocab)\n",
        "        print('data has %d words, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = {word: i for i, word in enumerate(vocab)}\n",
        "        self.itos = {i: word for i, word in enumerate(vocab)}\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = words\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every word to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbrebM87LyS1",
        "outputId": "c9ddd8d7-cbd9-42cf-a7bb-7e3088f73d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1987763 words, 34541 unique.\n"
          ]
        }
      ],
      "source": [
        "block_size = 128\n",
        "train_dataset = WordDataset(text, block_size) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1uNjn_KeL6Z1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ab817f-9bc7-42aa-dcf3-b0a2c023d3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, shuffle=True, pin_memory=True, batch_size=batch_size\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: this can take hours. I stopped when loss was around 0.5"
      ],
      "metadata": {
        "id": "okz-X-H07a9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_jks0WlMY13"
      },
      "outputs": [],
      "source": [
        "model = DecoderOnlyTransformer(\n",
        "    num_layers=8,\n",
        "    num_heads=8,\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    hidden_size=512,\n",
        "    max_pos_embeddings=train_dataset.block_size,\n",
        "    dropout=0.1,\n",
        ").to(device).train()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=6e-4)\n",
        "\n",
        "max_epochs = 1\n",
        "for epoch in range(max_epochs):\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"epoch {epoch} iter {it}: train loss {loss.item():.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hFgbF_ckMlT2"
      },
      "outputs": [],
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nsdA5WvFMpQs"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usqxe28CMvNT",
        "outputId": "7c3369c0-1b0e-443d-8d38-8ab68ab24a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " O God, O God! If there be but a shadow in the small\n",
            "thing more than you should endure the devil than you. You are a good\n",
            "chorus when ’a is so in the time of Pompey.\n",
            "\n",
            "WILLIAMS.\n",
            "I would have you see the King as I may.\n",
            "\n",
            "PISTOL.\n",
            "I understand thee not.\n",
            "\n",
            "FLUELLEN.\n",
            "I will give you bloody courage off and done; God help me to countenance!\n",
            "I feel to God his Grace of arms, and I hate’, I stand in Richard’s hall.\n",
            "O, that we now could come by first the King!\n",
            "\n",
            "GLOUCESTER.\n",
            "Never make him welcome with his lady’s words,\n",
            "When with his body’s right hand this favour now.\n",
            "This on his part I’ll charm this napkin:\n",
            "Do but consent; and presently return again,\n",
            "See when they bid me come my eyes to be.\n",
            "\n",
            "KING RICHARD.\n",
            "What I can do I will thankfully be;\n",
            "For loan oft loses not itself so near.\n",
            "\n",
            "QUEEN.\n",
            "As above the slow justice, and in preserve,\n",
            "With me, and with forgotten head,\n",
            "With heavy weight and modest breath doth lie,\n",
            "As the death of this happy womb.\n",
            "\n",
            "KING RICHARD.\n",
            "The same of them is too bold and worn,\n",
            "Unapt to stir at him with my crown.\n",
            "\n",
            "QUEEN.\n",
            "But God sort an hour to fight!\n",
            "\n",
            "GREEN.\n",
            "An champion that may make me present\n",
            "Of that faint fate to the world’s point.\n",
            "\n",
            "ROSS.\n",
            "I will, my liege.\n",
            "\n",
            " [_Exit._]\n",
            "\n",
            "LADY MACDUFF.\n",
            "Each man hath seen a poison’d thing.\n",
            "Where did they find, \n"
          ]
        }
      ],
      "source": [
        "context = \" O God, O God! \"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in re.split(r\"\\b\", context)], dtype=torch.long)[None,...].to(device)\n",
        "y = sample(model, x, 500, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd4xfpdorGboE4cDxskJZr",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}