{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzDVbVXkQ6F3fkXb9RK9oz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckkissane/deep_learning_curriculum/blob/master/solutions/7_Alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the ELK report (the first ~30 pages, which frame the problem) and try to come up with a solution of your own that handles all of the counterexamples in the report."
      ],
      "metadata": {
        "id": "fJD4rt88nM5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disclaimer: if you're looking for solutions it's probably more useful to reference the post [here](https://www.alignmentforum.org/posts/zjMKpSB2Xccn9qi5t/elk-prize-results). I'm just adding this page to my github for the sake of completeness."
      ],
      "metadata": {
        "id": "bYW-M2_w-O7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context: As of writing this, I've read up to the \"Ontology Identification\" section on page 36. "
      ],
      "metadata": {
        "id": "nASNPYDe4E_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My summary of previous attempts and their issues:\n",
        "1. Human feedback: (human can't reliably label data correctly for very complex actions)\n",
        "2. Train reporter that answers questions about simple human understandable (video1, action, video2) sequences, hope it generalizes: (reporter may learn to simulate observers responses instead of answering questions truthfully)\n",
        "3. Use human operator to label, who can choose complex actions that even a human observer wouldn't understand: (reporter can do perfect inference on a sufficiently diligent human, which leads to the same issues as the human simulator problem above)\n",
        "4. Use aligned AI assistants to automate science and explain what is going on: (Science is likely harder than SGD, so this is more expensive, and not competitive with bad reporters)\n",
        "5. Have reporter directly suggest new bayes net with imitative generalization: (ontological mismatch: human won't actually be able to understand the suggested bayes net since AI might use alien representations)\n"
      ],
      "metadata": {
        "id": "rbN2AaDT8BxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__New Strategy:__ Imitative generalization + penalize depending on too many parts of the predictor."
      ],
      "metadata": {
        "id": "xJ5L4S4Fll9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__How this solves the previous counterexample:__ The previous problem (ontological mismatch) was that the proposed change to the humans bayes net may not be understandable to the human, as the predictors representations may be too alien. In the best case, penalizing the reporter depending on too many parts of the predictor could incentivize the reporter to learn to distill the proposed change into fewer nodes that represent abstractions which the human (+ aligned AI assistants) could understand. After all, the questions that we care about (is the diamond in the room) are mostly simple, so intuitively it should be able to learn to propose a simpler bayes net."
      ],
      "metadata": {
        "id": "ymaSx4g4l_Q1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Counterexamples:__ \n",
        "* It's possible that the predictors representations are so fundamentally different to humans, that even using less nodes doesn't actually solve ontological mismatch.\n",
        "* Intuitively, this might require more iterations of gradient descent for the model to learn how to effectively distill at scale, (since the predictor bayes net can be arbitrarily big and complex) making this approach more computationally expensive. Thus, even if the penalties solve ontological mismatch in the best case, it may not be competitive with bad reporters. \n",
        "\n",
        "(there are probably more I haven't thought of yet)"
      ],
      "metadata": {
        "id": "rk0mwZwfmpib"
      }
    }
  ]
}